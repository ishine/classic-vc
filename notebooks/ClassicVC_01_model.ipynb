{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11113f4d",
   "metadata": {},
   "source": [
    "\n",
    "# ClassicVC model structure\n",
    "\n",
    "Lyodos 著\n",
    "\n",
    "Version 1.0.0 (2024-07-14)\n",
    "\n",
    "このノートブックでは、ClassicVC の PyTorch モデルをインスタンス化する方法を示す。\n",
    "あわせて各コンポーネントの解説を行う。\n",
    "なお開発者はプログラミングや信号処理の教育を受けたことがなく、本業は生物系（しかも野外で動物さん達を観察する）なので、\n",
    "用語や概念の解釈がしばしば的外れな点は容赦いただきたい。\n",
    "\n",
    "## 総論\n",
    "\n",
    "ClassicVC は連続潜在変数ベースで any-to-any 声質変換を行うことを目指して開発された。\n",
    "基本的な設計は AutoVC の流れをくむエンコーダ・デコーダモデルである。\n",
    "\n",
    "すなわちソース音声から content encoder で発話内容の埋め込みを取り出し、これを話者スタイルの時間次元を持たないベクトルと組み合わせ、\n",
    "デコーダに投入する。デコーダはいわゆるニューラルボコーダの構造を踏襲しており、潜在空間から直接 24000 Hz 音声を復元できる。\n",
    "訓練時は、発話内容と同じソース音声から得た話者スタイルを用いて自己再構成を行い、\n",
    "推論時は別のターゲット話者から得た話者スタイルと合わせて、そのターゲットの声質を再現する。\n",
    "\n",
    "また、最初期の研究例と現代の VC モデルとの違いとして、基本ピッチ（F0）をソース音声から推定し、その値をデコーダに教えることで、\n",
    "音高変化を自由に制御することが可能になっている。\n",
    "ClassicVC ではさらに、ソース音声の F0 を基準としてプラスマイナスいくつで音高をシフトさせるか、\n",
    "あるいは発話内容とターゲット話者スタイルの組み合わせに基づき、ターゲットが話すであろう F0 を推定するかという、\n",
    "音高のベースラインの選択が可能である。\n",
    "\n",
    "これらの強力な調整機能は、いくつかの主要なコンポーネントを StyleTTS 2 (Li et al. 2023 https://arxiv.org/abs/2306.07691) \n",
    "から踏襲したことで可能となった。\n",
    "モデルの[ソースコード](https://github.com/yl4579/StyleTTS2)をオープンソースで公開してくださっている著者の方々に、\n",
    "あらめて謝意を表する。\n",
    "ただしこのモデルは名前のとおり TTS (text to speech) 用であるから、VC タスクを実現するために細かな差し替えを独自に行った。\n",
    "概要を以下に示す。\n",
    "\n",
    "### コンポーネント概要図\n",
    "\n",
    "![コンポーネント概要図](https://github.com/lyodos/classic-vc/raw/main/docs/ClassicVC_overview-en.png)\n",
    "\n",
    "なお、元になった StyleTTS 2 の概要図も以下に示す。\n",
    "TTS よりも VC の方が（phoneme duration を推定しなくていいため）かなり構造が簡単になることがお分かりだろう。\n",
    "また StyleTTS 2 の核心部分である style diffusion 拡散モデルについては、訓練にも推論にも時間を使うため削除した。\n",
    "\n",
    "![StyleTTS 2 overview 1](https://github.com/lyodos/classic-vc/raw/main/docs/styleTTS2_01.png)\n",
    "\n",
    "![StyleTTS 2 overview 2](https://github.com/lyodos/classic-vc/raw/main/docs/styleTTS2_02.png)\n",
    "\n",
    "![StyleTTS 2 overview 3](https://github.com/lyodos/classic-vc/raw/main/docs/styleTTS2_03.png)\n",
    "\n",
    "見てのとおり StyleTTS 2 はモデル全体が巨大なため、訓練は two-stage で実施される。\n",
    "ClassicVC は幸い、そこまで大きくないので VRAM 24 GB の GPU を 1 台使い、single-stage で訓練が可能である。\n",
    "\n",
    "では、さっそくモデルの各コンポーネントをノートブック上で定義していこう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270592b",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 準備\n",
    "\n",
    "データを置くディレクトリと、使用する GPU を指定する。\n",
    "\n",
    "プロジェクト（ClassicVC）全体で重量級のデータを共有するフォルダは、アクセスが高速なストレージに置く必要がある。\n",
    "ClassicVC のリポジトリのクローン自体（および、このノートブック）は別の場所に配置してもいい。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaacac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:40.797633Z",
     "start_time": "2024-07-14T12:18:40.792704Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "# チェックポイントやログを保存する、機械学習関連のデータを置くルートディレクトリの指定\n",
    "\n",
    "DATASET_ROOT_PATH = Path(\"/home/lyodos/study/dataset\") # このフォルダ名はユーザーの実情に合わせて書き変えること\n",
    "\n",
    "proj_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\"\n",
    "proj_path.mkdir(parents = True, exist_ok = True)\n",
    "print(\"Project directory:\", str(proj_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08053519",
   "metadata": {},
   "source": [
    "\n",
    "モデルの評価に使う GPU を指定する。現在マルチ GPU には対応していない。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272a13d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:41.566514Z",
     "start_time": "2024-07-14T12:18:40.798685Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    device = torch.device('cuda:1') # 複数ある場合、よわよわの GPU を評価用に使う（強いほうは訓練用に空ける）\n",
    "elif torch.cuda.device_count() >= 1:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b504eae",
   "metadata": {},
   "source": [
    "\n",
    "以下、ネットワークの部品を定義していく。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9161ea",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## HarmoF0 pitch tracker の定義\n",
    "\n",
    "このモデルは Wei et al (2022) https://doi.org/10.1109/ICME52920.2022.9858935 で提案された。\n",
    "\n",
    "> HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation\n",
    "> Weixing Wei, Peilin Li, Yi Yu, Wei Li\n",
    "\n",
    "HarmoF0 公式実装および訓練済みの重みは https://github.com/WX-Wei/HarmoF0 において、MIT License で公開されている。\n",
    "ClassicVC ではこれらを元ライセンスに準拠して転載しているが、ONNX への変換のためにコードを部分的に書き変えている（重みは転用可能）。\n",
    "\n",
    "以下のコンポーネントにソース音声を投入して、時間フレーム単位の基本ピッチ（F0）、音量（energy）、アクティベーション（現在鳴っているのが人声か背景雑音か）を推定する。あわせて、推定過程で必要となる対数スペクトログラムも取り出す。\n",
    "\n",
    "* 入力は 16000 Hz waveform で、ホップで 1/160 になるので 10 ms 解像度である。\n",
    "\n",
    "* 入力は `torch.Size([n_batch, n_sample])` ただし `(1, 513)` （32.1 ms）以上の wave length がないとエラーになる。\n",
    "\n",
    "### Wav2spec モジュール\n",
    "\n",
    "実際の HarmoF0 ネットワーク（畳み込みネットワーク）への入力は、対数スペクトログラムの形で行われる。\n",
    "ただし `torch.stft` や `librosa.stft` 等のコードを使うと、ONNX に変換するときに複素数に対応していないという理由でエラーが出る。\n",
    "\n",
    "なので wav2spec の部分だけは Prem Seetharaman による \"STFT/iSTFT in PyTorch\" https://github.com/pseeth/pytorch-stft を採用し、\n",
    "古典的な短時間フーリエ変換を畳み込みに置換したモジュール（このリポジトリの `conv_stft.py`）を使用する。\n",
    "これにより、waveform を放り込んで F0 やスペクトログラムを取り出すお手軽機能が、可搬性の高い ONNX 形式で使えるようになる。\n",
    "\n",
    "> なお、`conv_stft.py` のコードだけは元実装に従い、MIT ではなくBSD 3-Clause License でリリースする。\n",
    "それぞれのソースファイル冒頭にライセンスが記載されているので、確認してほしい。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4fbca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:42.624230Z",
     "start_time": "2024-07-14T12:18:41.567612Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import sys\n",
    "sys.path.append('../') # ClassicVC のリポジトリのルートをパスに入れて、model ディレクトリを探せるようにしている\n",
    "\n",
    "from model.harmof0.pitch_tracker import BatchedPitchEnergyTracker\n",
    "\n",
    "def pred_f0_len(length):\n",
    "    return length // 160 + 1\n",
    "\n",
    "harmof0_tracker = BatchedPitchEnergyTracker(\n",
    "    checkpoint_path = \"../model/harmof0/checkpoints/mdb-stem-synth.pth\", # HarmoF0 作者による訓練済みの重みを再配布\n",
    "    fmin = 27.5, # f0 として想定する最低周波数の Hz で、ピアノの最低音の A に相当する。\n",
    "    sample_rate = 16000,\n",
    "    hop_length = 160, # f0 を推定する間隔。160/16000 = 10 ms \n",
    "    frame_len = 1024, # sliding window を切り出す長さ\n",
    "    frames_per_step = 1000, # 1 回の forward で投入する最大セグメント数\n",
    "    high_threshold = 0.8, \n",
    "    low_threshold = 0.1, \n",
    "    freq_bins_in = 88*4,\n",
    "    bins_per_octave_in = 48,\n",
    "    bins_per_octave_out = 48,\n",
    "    device = device,\n",
    "    compile = False,\n",
    "    dry_run = 10, \n",
    ")\n",
    "\n",
    "# ちなみに初期化時点でネットワークの重みは freeze 済み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9bf199",
   "metadata": {},
   "source": [
    "\n",
    "なお、コンポーネントが正常に動作していることを確かめるために適当な音声を突っ込んでみる。\n",
    "以下のサンプルは、VCTK コーパス (The Centre for Speech Technology Research, University of Edinburgh) から Open Data Commons Attribution License (ODC-By) v1.0. に従い転載した。\n",
    "\n",
    "> なお VCTK コーパスは ClassicVC の訓練には一切用いておらず（Notebook 02 も参照）、ここでは未知話者への適用（zero-shot）となる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05b7b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:42.973872Z",
     "start_time": "2024-07-14T12:18:42.626146Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VCTK コーパスからのボイスサンプルを読み込んでみる\n",
    "debug = True\n",
    "if debug == True:  \n",
    "    waveform, orig_sr = torchaudio.load('../wavs/p225_003.wav') # まずオリジナル周波数でロード\n",
    "    wav16 = torchaudio.transforms.Resample(orig_freq = orig_sr, new_freq = 16000)(waveform).to(device)\n",
    "    with torch.no_grad():\n",
    "        %time freq_t, act_t, energy_t, spec = harmof0_tracker(wav16) # 初回だけ遅いので 2 回通して時間を測る\n",
    "        %time freq_t, act_t, energy_t, spec = harmof0_tracker(wav16)\n",
    "    print(freq_t.shape, act_t.shape, energy_t.shape, spec.shape)\n",
    "\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(waveform, rate = orig_sr, normalize = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbe9a7",
   "metadata": {},
   "source": [
    "\n",
    "ネットワークの構造も調査しておく\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37452e3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:43.083125Z",
     "start_time": "2024-07-14T12:18:42.974741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "with torch.no_grad():\n",
    "    %time pitch_map = harmof0_tracker.single_tracker.net(spec)\n",
    "print(pitch_map.shape)\n",
    "\n",
    "torchinfo.summary(model = harmof0_tracker.single_tracker.net, input_size = spec.shape, depth = 4, \n",
    "                  col_names=[\"input_size\", \"output_size\", \"num_params\"], device = device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acd729",
   "metadata": {},
   "source": [
    "\n",
    "実際に声のピッチとエネルギーをプロットしてみる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071317c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:46.025920Z",
     "start_time": "2024-07-14T12:18:43.084413Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_example = True\n",
    "\n",
    "if plot_example:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (16.0, 6.0) # default 6.4, 4.8\n",
    "\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    axis.set_title(\"F0 Feature\")\n",
    "    axis.grid(True)\n",
    "\n",
    "    # x 軸（時間軸）のセット\n",
    "    end_time = waveform.shape[-1] / orig_sr\n",
    "    axis.set_ylim((-1.3, 1.3))\n",
    "    time_axis = torch.linspace(start = 0, end = end_time, steps = waveform.shape[-1]) # steps は等差数列の要素数\n",
    "\n",
    "    # 音声波形を第 1 軸にプロット\n",
    "    ln1 = axis.plot(\n",
    "        time_axis, \n",
    "        waveform.detach().squeeze().cpu().numpy(), \n",
    "        linewidth = 1, \n",
    "        color = 'gray', \n",
    "        label = 'Waveform', \n",
    "        alpha = 0.3,\n",
    "    )\n",
    "\n",
    "    axis2 = axis.twinx()\n",
    "    axis2.set_ylim((0, 600))\n",
    "\n",
    "    # HarmoF0 pitch sequence\n",
    "    time_axis = torch.linspace(start = 0, end = end_time, steps = freq_t.shape[-1])\n",
    "    ln4 = axis2.plot(\n",
    "        time_axis, \n",
    "        freq_t.squeeze().detach().cpu().numpy(), \n",
    "        linewidth = 1, \n",
    "        linestyle = \"dashed\",\n",
    "        label = 'Frequency (HarmoF0)', \n",
    "        color = \"black\",\n",
    "    )\n",
    "\n",
    "    # HarmoF0 activation sequence\n",
    "    time_axis = torch.linspace(start = 0, end = end_time, steps = act_t.shape[-1])\n",
    "    ln5 = axis.plot(\n",
    "        time_axis, \n",
    "        act_t.squeeze().detach().cpu().numpy(),\n",
    "        linewidth = 1, \n",
    "        label = \"Activation (HarmoF0)\", \n",
    "        color = cm.hsv(0.7),\n",
    "    )\n",
    "    \n",
    "    # HarmoF0 energy sequence\n",
    "    time_axis = torch.linspace(start = 0, end = end_time, steps = act_t.shape[-1])\n",
    "    ln3 = axis.plot(\n",
    "        time_axis, \n",
    "        energy_t.squeeze().detach().cpu().numpy() / 10, \n",
    "        linewidth = 1, \n",
    "        label = \"Energy (HarmoF0) / 10\", \n",
    "        color = cm.hsv(0.9),\n",
    "    )\n",
    "\n",
    "    # 合成\n",
    "    lns = ln1 + ln4 + ln5 + ln3\n",
    "    labels = [l.get_label() for l in lns] # 重複回避のためラベルを結合してから描画する\n",
    "    axis.legend(lns, labels, loc = 'upper right')\n",
    "\n",
    "    plt.show(block = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3aafc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:46.154302Z",
     "start_time": "2024-07-14T12:18:46.027087Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from model.utils import plot_spectrogram_harmof0\n",
    "\n",
    "plot_spectrogram_harmof0(\n",
    "    spec.squeeze().cpu()[48:, :],\n",
    "    f0 = freq_t.detach().cpu(), \n",
    "    act = act_t.detach().cpu(), \n",
    "    size = (12, 4.5),\n",
    "    aspect = None,\n",
    "    vmin = -50,\n",
    "    vmax = 40,\n",
    "    cmap = \"inferno\",\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e0760",
   "metadata": {},
   "source": [
    "\n",
    "青い破線が activation である。概ね声がある場所で上側（1）に張り付いていることが分かるだろう。\n",
    "\n",
    "ちなみにスペクトログラムの下側 48 bins を捨ててから図示しているが、これは低音側 1 オクターブ（27.5 Hz ～ 55 Hz）\n",
    "に人声が存在しないだろうとの想定から、後工程のネットワークに明らかに不要な周波数帯を与えないことで軽量化する工夫である。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424908a",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## (Acoustic) Style Encoder の初期化\n",
    "\n",
    "$E_{a}(x)$ を作成する。構造は StyleTTS 2 のものを、入力の feature dimension を HarmoF0 に合うよう改変して採用している。\n",
    "なので既存の重みは使用できず、自分で作る必要がある。\n",
    "\n",
    "入力はスペクトログラムで、さらに `(batch, 1, dim_spec, n_frame >= 80)` の 4D テンソルでないと受けられない。\n",
    "なので上で作った spec を unsqueeze(1) して入れることになる。\n",
    "\n",
    "また HarmoF0 の低音側は 27.5 Hz だが、下の 1 オクターブ（48 bins）は人の声で使うことはなく無駄なので、\n",
    "入力は 352 ではなく 352 - 48 = 304 にする。これでパラメータ数を 23,719,264 → 23,189,104  に削減できる。\n",
    "\n",
    "出力は時間次元を持たない `(batch, 128)`。\n",
    "\n",
    "\n",
    "> このモデル、そこそこの計算時間を要する。VRAM に余裕があれば Transformer ベースもしくは状態空間モデルベースで、\n",
    "同等機能を持つものを開発して置き換えるとよいだろう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2577af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:46.243094Z",
     "start_time": "2024-07-14T12:18:46.155618Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.StyleTTS2.models import StyleEncoder\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StyleEncoderConfig:\n",
    "    dim_in: int = 304\n",
    "    style_dim: int = 128\n",
    "    max_conv_dim: int = 512\n",
    "\n",
    "style_encoder_cfg = OmegaConf.structured(StyleEncoderConfig())\n",
    "\n",
    "style_encoder = StyleEncoder(\n",
    "    dim_in = style_encoder_cfg.dim_in, # 304\n",
    "    style_dim = style_encoder_cfg.style_dim, # 128\n",
    "    max_conv_dim = style_encoder_cfg.max_conv_dim, # 512\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee344f02",
   "metadata": {},
   "source": [
    "\n",
    "重みを読み込む場合は以下のようにする。\n",
    "以下の \"style_encoder.pth\" は Notebook 02 で準備するデータセットを、Notebook 03 に示すコードで訓練したネットワークの state_dict である。こちらもリポジトリの他の構成要素（一部除く）と同じく、MIT License で配布される。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfaf787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:46.286353Z",
     "start_time": "2024-07-14T12:18:46.244156Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# この場所に 作った重みを 置いておく\n",
    "style_dict_path = \"../weights/style_encoder.pth\"\n",
    "\n",
    "style_dict = torch.load(style_dict_path, map_location = device)\n",
    "style_encoder.load_state_dict(style_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba1805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:46.481566Z",
     "start_time": "2024-07-14T12:18:46.287981Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "with torch.no_grad():    \n",
    "    %time s_a = style_encoder(spec[:, 48:, :].unsqueeze(1))\n",
    "print(s_a.shape)\n",
    "\n",
    "torchinfo.summary(model = style_encoder, input_size = spec[:, 48:, :].unsqueeze(1).shape, depth = 4, \n",
    "                  col_names=[\"input_size\", \"output_size\", \"num_params\"], device = device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c87e86",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Content encoder (ContentVec)\n",
    "\n",
    "音声からの発話内容の抽出に使うのが、Quian et al. (2022) による ContentVec https://doi.org/10.48550/arXiv.2204.09224 である。\n",
    "こちらも開発者による実装と重みが、以下のリポジトリにおいて MIT License で公開されている。\n",
    "\n",
    "> Kaizhi Qian, Yang Zhang, Heting Gao, Junrui Ni, Cheng-I Lai, David Cox, Mark Hasegawa-Johnson, Shiyu Chang\n",
    "> ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers\n",
    "\n",
    "https://proceedings.mlr.press/v162/qian22b.html\n",
    "\n",
    "https://arxiv.org/abs/2204.09224\n",
    "\n",
    "https://github.com/auspicious3000/contentvec\n",
    "\n",
    "\n",
    "ContentVec のネットワーク構造を定義するには、transformers パッケージの HubertModel を使う。\n",
    "公式で配布されているコードを使うと、ONNX 化できないためである。詳しくは Notebook 04 で解説する。\n",
    "\n",
    "> 重みは ContentVec の公式で配布している ContentVec_legacy の 500 class である。\n",
    "> ただし若干アレンジする必要があるので、Notebook 04 に書いてある手順を参照して用意すること。\n",
    "> 以下には最終成果物のモデルをロードする手順だけを示す。\n",
    "\n",
    "* 入力は `torch.Size([n_batch, n_sample])` の 16 khz mono waveform だが、 `(1, 400)` （25 ms）以上ないとエラーになる。\n",
    "\n",
    "* 隠れ層の最終次元を出力として使用し、そのサイズは `torch.Size([batch, ((length - 80) // 320), 768])` となる。\n",
    "\n",
    "* HubertModel の生出力は feature last であるが、ほとんどの下流工程は time last になるので、転置して使うことに注意。\n",
    "\n",
    "入力が 16000 Hz で 1/320 に間引くので **ContentVec の出力テンソルの hop は 20 ms である**。\n",
    "\n",
    "* HarmoF0 pitch tracker は 10 ms hop なので、ちょうど端数以外は 2 倍のテンソルサイズとなる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf01b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:49.709251Z",
     "start_time": "2024-07-14T12:18:46.482416Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import HubertConfig, HubertModel\n",
    "\n",
    "CE = HubertModel(HubertConfig())\n",
    "\n",
    "contentvec_path = proj_path / \"contentvec_500_hubert.pth\"\n",
    "\n",
    "CE_dict = torch.load(str(contentvec_path), map_location = torch.device('cpu'))\n",
    "CE.load_state_dict(CE_dict, strict = True)\n",
    "CE.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841789af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:49.774373Z",
     "start_time": "2024-07-14T12:18:49.710334Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    waveform, orig_sr = torchaudio.load('../wavs/p225_003.wav') # まずオリジナル周波数でロード\n",
    "    wav16 = torchaudio.transforms.Resample(orig_freq = orig_sr, new_freq = 16000)(waveform).to(device)\n",
    "    %time content = CE(wav16, output_hidden_states = True)\n",
    "\n",
    "print(type(content))\n",
    "print(len(content))\n",
    "print(content.keys())\n",
    "print(content[\"last_hidden_state\"].shape) # テンソルを取り出すには [\"last_hidden_state\"] キーへのアクセスが必要なので注意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c695b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.089692Z",
     "start_time": "2024-07-14T12:18:49.775242Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "torchinfo.summary(model = CE, input_size = wav16.shape, depth = 4, \n",
    "                  col_names=[\"input_size\", \"output_size\", \"num_params\"], device = \"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856dc48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.219380Z",
     "start_time": "2024-07-14T12:18:50.090661Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(content[\"last_hidden_state\"].cpu().squeeze().T, vmin = -1, vmax = 1)\n",
    "fig.colorbar(im, ax = ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cea655",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## ProsodyPredictor の定義\n",
    "\n",
    "ContentVec の抽出した特徴量と、Style Encoder で抽出した話者スタイル $s_a$ をもとに、\n",
    "F0 と energy の時間変化を予測するネットワークである。\n",
    "\n",
    "原型となったネットワークは StyleTTS 2 に ProsodyPredictor として存在するが、\n",
    "やはり ONNX 化が止まらないようにマイナーチェンジした `F0NPredictorAll` を作った。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8e4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.340970Z",
     "start_time": "2024-07-14T12:18:50.220297Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.StyleTTS2.models import F0NPredictorAll\n",
    "\n",
    "@dataclass\n",
    "class PrododyPredictorConfig:\n",
    "    style_dim: int = 128\n",
    "    hidden_dim: int = 768\n",
    "    n_layer: int = 3\n",
    "    dropout: float = 0.2\n",
    "\n",
    "prosody_predictor_cfg = OmegaConf.structured(PrododyPredictorConfig())\n",
    "\n",
    "f0n_predictor = F0NPredictorAll(\n",
    "    style_dim = prosody_predictor_cfg.style_dim,\n",
    "    d_hid = prosody_predictor_cfg.hidden_dim,\n",
    "    nlayers = prosody_predictor_cfg.n_layer,\n",
    "    dropout = prosody_predictor_cfg.dropout,\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddb18b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.392128Z",
     "start_time": "2024-07-14T12:18:50.341954Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "f0n_dict_path = \"../weights/f0n_predictor.pth\"\n",
    "\n",
    "f0n_dict = torch.load(f0n_dict_path, map_location = device)\n",
    "f0n_predictor.load_state_dict(f0n_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb30fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.526555Z",
     "start_time": "2024-07-14T12:18:50.393098Z"
    }
   },
   "outputs": [],
   "source": [
    "# 新造した F0NPredictorAll は time last で入れる。返り値はそれぞれ feature 次元が潰れた 2D の time last だが、時間解像度が 2 倍\n",
    "\n",
    "with torch.no_grad():\n",
    "    %time pred_F0, pred_N = f0n_predictor(content[\"last_hidden_state\"].transpose(2, 1), s_a)\n",
    "    %time pred_F0, pred_N = f0n_predictor(content[\"last_hidden_state\"].transpose(2, 1), s_a)\n",
    "\n",
    "print(pred_F0.shape, pred_N.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df564bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.611611Z",
     "start_time": "2024-07-14T12:18:50.527707Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torchinfo.summary(\n",
    "    model = f0n_predictor, \n",
    "    input_size = (content[\"last_hidden_state\"].transpose(2, 1).shape, s_a.shape), \n",
    "    depth = 3, \n",
    "    col_names = [\"input_size\", \"output_size\", \"num_params\"], \n",
    "    device = device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b87499",
   "metadata": {},
   "source": [
    "\n",
    "`content` は 20 ms 間隔だが、`pred_F0`, `pred_N` は 10 ms 間隔である。\n",
    "\n",
    "また上述のとおり HarmoF0 についても返り値は 10 ms 間隔だが、端数があるため 1 フレーム増えることがある。\n",
    "\n",
    "なお次の decoder の入力系列長は、content を基準として 1/50 s = 20 ms 間隔で計算するシステムを採用している。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad3a8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# VC Decoder の定義\n",
    "\n",
    "StyleTTS 2 には iSTFTNet ベースのデコーダもあるが、スピードと音質を考えて、HiFi-GAN ベースだけをとりあえず考える。\n",
    "\n",
    "> このデコーダの定義は RVC 等でも用いられている SourceModuleHnNSF に準拠しているようだが、\n",
    "どうやら細部が異なる。多分 StyleTTS 2 で採用される以前に、元となった実装があるのだと思う。\n",
    "\n",
    "```python\n",
    "y_rec = decoder(\n",
    "    en, # content 情報 torch.Size([1, 512, 445]) # 最終次元が長さ依存\n",
    "    F0_real, # F0 torch.Size([1, 890]) # content 特徴量の長さの 2 倍\n",
    "    real_norm, # Energy torch.Size([1, 890]) # content 特徴量の長さの 2 倍\n",
    "    s, # acoustic style は固定サイズ torch.Size([1, 128])\n",
    ")\n",
    "```\n",
    "\n",
    "上に書いたとおり decoder は基準長（content を使う）が 1/50 s = 20 ms ピッチである。\n",
    "これをそのまま ConvTranspose で伸長して 24000 Hz 音声にしようとすると、24000/50 = 480 倍となる。\n",
    "\n",
    "* なお内部定義的に、 `upsample_rates` の積和のさらに 2 倍が実伸長率になるので注意。\n",
    "\n",
    "* StyleTTS 2 は `upsample_rates = [5, 5, 3, 2] × 2` だったので、そのままだと 300 倍になってしまう。\n",
    "\n",
    "* なので ClassicVC ではとりあえず `upsample_rates = [10, 4, 3, 2] × 2` で 480 倍に変更した。\n",
    "\n",
    "本当は音声を 48k 化したいが手が足りていない。 `upsample_rates = [10, 4, 3, 2, 2] × 2` で 960 倍にするのだと思うが、まだ安定して訓練できない。\n",
    "\n",
    "> 一応アイデアとしては既存の [10, 4, 3, 2] アップサンプル部分までのネットワーク層について、24k 用に公開している重みを移植してその部分は凍結し、\n",
    "新しく足した最後の 2 倍のアップサンプルだけを、話者数は少なくてもいいので高音質の音声データセットで訓練できるだろう。\n",
    "暇な人はやってみて欲しい。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce9c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:50.959236Z",
     "start_time": "2024-07-14T12:18:50.612625Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "import math\n",
    "\n",
    "from model.StyleTTS2.hifigan import Decoder\n",
    "\n",
    "upsample_rate_list = [10, 4, 3, 2]\n",
    "\n",
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    sampling_rate: int = 24000\n",
    "    dim_in: int = 768\n",
    "    style_dim: int = 128\n",
    "    upsample_rate_list: list = tuple(upsample_rate_list)\n",
    "    upsample_kernel_list: list = tuple([i*2 for i in upsample_rate_list])\n",
    "    upsample_total: int = math.prod(upsample_rate_list)*2\n",
    "    upsample_initial_channel: int = 512\n",
    "    harmonic_num: int = 8\n",
    "\n",
    "decoder_cfg = OmegaConf.structured(DecoderConfig())\n",
    "\n",
    "decoder = Decoder(\n",
    "    sampling_rate = decoder_cfg.sampling_rate,\n",
    "    dim_in = decoder_cfg.dim_in,\n",
    "    style_dim = decoder_cfg.style_dim,\n",
    "    resblock_kernel_sizes = [3, 7, 11], # ここは大多数のモデルで同じ設定値を採用している\n",
    "    resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], # ここは大多数のモデルで同じ設定値を採用している\n",
    "    upsample_rates = decoder_cfg.upsample_rate_list,\n",
    "    upsample_initial_channel = decoder_cfg.upsample_initial_channel,\n",
    "    upsample_kernel_sizes = decoder_cfg.upsample_kernel_list,\n",
    "    harmonic_num = decoder_cfg.harmonic_num,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1367da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T13:51:33.905049Z",
     "start_time": "2024-03-17T13:51:33.902675Z"
    }
   },
   "source": [
    "\n",
    "実は注意が必要で、\n",
    "`F0_real`, `real_norm` は運用時は偶数のフレーム長でなければならない。\n",
    "さもないと content が // 2 で短くなってしまうため、割り切れない長さになり、decoder 内の AdainResBlk1d でエラーが出る。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e6bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:51.076234Z",
     "start_time": "2024-07-14T12:18:50.960268Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "decoder_dict_path = \"../weights/decoder.pth\"\n",
    "\n",
    "decoder_dict = torch.load(decoder_dict_path, map_location = device)\n",
    "decoder.load_state_dict(decoder_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316bbfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:51.490310Z",
     "start_time": "2024-07-14T12:18:51.077335Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(content[\"last_hidden_state\"].shape)\n",
    "print(pred_F0.shape, pred_N.shape)\n",
    "print(s_a.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    %time reconst = decoder(content[\"last_hidden_state\"].transpose(2, 1), pred_F0, pred_N, s_a)\n",
    "print(reconst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f677f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:51.802065Z",
     "start_time": "2024-07-14T12:18:51.491500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torchinfo.summary(\n",
    "    model = decoder, \n",
    "    input_size = (\n",
    "        content[\"last_hidden_state\"].transpose(2, 1).shape, pred_F0.shape, pred_N.shape, s_a.shape), \n",
    "    depth = 3, \n",
    "    col_names = [\"input_size\", \"output_size\", \"num_params\"], \n",
    "    device = device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff2710",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 非リアルタイムで音声ファイルを声質変換する手順\n",
    "\n",
    "音声から計算される特徴量の長さがフレーム単位で微妙に異なるので、\n",
    "末尾から一定サイズを切り取る形で利用する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054f2ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:51.810195Z",
     "start_time": "2024-07-14T12:18:51.803179Z"
    }
   },
   "outputs": [],
   "source": [
    "# ネットワークは上で定義した既存インスタンスを使っているので注意\n",
    "\n",
    "def pred_f0_len(length):\n",
    "    return length // 160 + 1\n",
    "\n",
    "def VC(\n",
    "    source_wav,\n",
    "    target_wav = None,\n",
    "    device = \"cuda:0\",\n",
    "    reconst: bool = False,\n",
    "    pred_prosody: bool = True,\n",
    "    pitch: float = 0.0,\n",
    "    return_originals: bool = False, # ソースおよびターゲットの waveform とサンプリング周波数も返す\n",
    "    debug: bool = False,\n",
    "):\n",
    "    wav_s, s_sr = torchaudio.load(source_wav) # まずオリジナル周波数でロード\n",
    "    w24_s = torchaudio.transforms.Resample(orig_freq = s_sr, new_freq = 24000)(wav_s)\n",
    "    w16_s = torchaudio.transforms.Resample(orig_freq = s_sr, new_freq = 16000)(wav_s)\n",
    "    seq_len = pred_f0_len(w16_s.size(-1)) # HarmoF0 が返すであろう系列長を計算しておく\n",
    "    if debug:\n",
    "        print(\"(input):\", wav_s.shape, s_sr, w16_s.shape, seq_len)\n",
    "    \n",
    "    if target_wav is None:\n",
    "        reconst = True\n",
    "        if debug:\n",
    "            print(\"'target_wav' is not specified. Reconstruction task will run.\")\n",
    "    else:\n",
    "        wav_t, t_sr = torchaudio.load(target_wav) # まずオリジナル周波数でロード\n",
    "        w24_t = torchaudio.transforms.Resample(orig_freq = t_sr, new_freq = 24000)(wav_t)\n",
    "        w16_t = torchaudio.transforms.Resample(orig_freq = t_sr, new_freq = 16000)(wav_t)\n",
    "\n",
    "    if reconst:\n",
    "        with torch.no_grad():\n",
    "            pitch_s, energy_s, act_s, spec_s = harmof0_tracker.to(device)(w16_s.to(device))\n",
    "            acoustic_style_s = style_encoder.to(device)(spec_s[:, 48:, :].unsqueeze(1))\n",
    "            prosodic_style_s = acoustic_style_s\n",
    "            content_s = CE.to(device)(w16_s.to(device))[\"last_hidden_state\"].transpose(2, 1)\n",
    "            if debug:\n",
    "                print(\"(reconst is True):\", acoustic_style_s.shape, prosodic_style_s.shape, content_s.shape)\n",
    "            if pred_prosody:\n",
    "                pred_F0_s, pred_N_s = f0n_predictor.to(device)(content_s, prosodic_style_s) \n",
    "                if debug:\n",
    "                    print(\"(pred_prosody is True):\", pred_F0_s.shape, pred_N_s.shape) \n",
    "                result = decoder.to(device)(\n",
    "                    content_s, \n",
    "                    pred_F0_s, \n",
    "                    pred_N_s, \n",
    "                    acoustic_style_s,\n",
    "                )\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"(pred_prosody is False):\", content_s.shape, pitch_s.shape) \n",
    "                result = decoder.to(device)(\n",
    "                    content_s, \n",
    "                    pitch_s[:, -content_s.shape[-1]*2:] * 2**(pitch/12), # 予測 pitch の方が長い（正確には content が短い）\n",
    "                    energy_s[:, -content_s.shape[-1]*2:], \n",
    "                    acoustic_style_s,\n",
    "                )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            pitch_s, energy_s, act_s, spec_s = harmof0_tracker.to(device)(w16_s.to(device))\n",
    "            pitch_t, energy_t, act_t, spec_t = harmof0_tracker.to(device)(w16_t.to(device))\n",
    "            acoustic_style_t = style_encoder.to(device)(spec_t[:, 48:, :].unsqueeze(1))\n",
    "            prosodic_style_t = acoustic_style_t\n",
    "            content_s = CE.to(device)(w16_s.to(device))[\"last_hidden_state\"].transpose(2, 1)\n",
    "            if debug:\n",
    "                print(\"(reconst is False):\", acoustic_style_t.shape, prosodic_style_t.shape, content_s.shape) \n",
    "            if pred_prosody:\n",
    "                pred_F0_st, pred_N_st = f0n_predictor.to(device)(content_s, prosodic_style_t)\n",
    "                if debug:\n",
    "                    print(\"(pred_prosody is True):\", pred_F0_st.shape, pred_N_st.shape) \n",
    "                result = decoder.to(device)(\n",
    "                    content_s, \n",
    "                    pred_F0_st * 2**(pitch/12), \n",
    "                    pred_N_st, \n",
    "                    acoustic_style_t,\n",
    "                )\n",
    "            else:\n",
    "                pitch_s, energy_s, act_s, _ = harmof0_tracker.to(device)(w16_s.to(device))\n",
    "                if debug:\n",
    "                    print(\"(pred_prosody is False):\", content_s.shape, pitch_s.shape) \n",
    "                result = decoder.to(device)(\n",
    "                    content_s, \n",
    "                    pitch_s[:, -content_s.shape[-1]*2:] * 2**(pitch/12), \n",
    "                    energy_s[:, -content_s.shape[-1]*2:], \n",
    "                    acoustic_style_t,\n",
    "                )\n",
    "        \n",
    "    if result.max() > 1 or result.min() < -1:\n",
    "        print(f\"Range of the synthesized waveform ({result.min()}, {result.max()}) exceeds [-1, 1] and clamped.\")\n",
    "        result = torch.clamp(result, -1, 1)\n",
    "\n",
    "    if return_originals:\n",
    "        if reconst:\n",
    "            return result, w24_s, s_sr\n",
    "        else:\n",
    "            return result, w24_s, s_sr, w24_t, t_sr\n",
    "    else:\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5a2dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:53.635469Z",
     "start_time": "2024-07-14T12:18:51.811083Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "source_link = '../wavs/p225_003.wav'\n",
    "target_link = '../wavs/p227_001.wav'\n",
    "\n",
    "# VC を実行。ピッチを変換先スタイルに合わせる\n",
    "wav_vc, wav_source, sr_source, wav_target, sr_target = VC(\n",
    "    source_link, \n",
    "    target_link, \n",
    "    reconst = False, \n",
    "    pred_prosody = True, \n",
    "    return_originals = True, \n",
    "    debug = True,\n",
    ")\n",
    "\n",
    "# target を指定しない → 再構成タスク\n",
    "wav_vc, wav_source, sr_source = VC(\n",
    "    source_link, \n",
    "    reconst = False, \n",
    "    pred_prosody = True, \n",
    "    return_originals = True, \n",
    "    debug = True,\n",
    ")\n",
    "\n",
    "# target はあるが、再構成を指定。またピッチを変換先ではなくソース音声の話者スタイルに合わせる\n",
    "wav_vc, wav_source, sr_source = VC(\n",
    "    source_link, \n",
    "    target_link, \n",
    "    reconst = True, \n",
    "    pred_prosody = False, \n",
    "    return_originals = True, \n",
    "    debug = True,\n",
    ")\n",
    "\n",
    "# オリジナルを返さない。ピッチを元音声から +6 半音上げる\n",
    "wav_vc = VC(\n",
    "    source_link, \n",
    "    target_link, \n",
    "    reconst = False, \n",
    "    pred_prosody = False, \n",
    "    pitch = 6.0,\n",
    "    return_originals = False, \n",
    "    debug = True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c5028",
   "metadata": {},
   "source": [
    "\n",
    "以下は変換前後の音声を Notebook 上で再生するための便利関数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cfadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:53.639322Z",
     "start_time": "2024-07-14T12:18:53.636597Z"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "def display_audio_grid(\n",
    "    wavs,\n",
    "    titles,\n",
    "    sr = decoder_cfg.sampling_rate,\n",
    "):\n",
    "    nrow = len(wavs)\n",
    "\n",
    "    # 音声データとタイトルのリスト\n",
    "    audio_html = \"\"\n",
    "    for wav_row, title_row in zip(wavs, titles):\n",
    "        wav_row = [w.detach().cpu().squeeze().numpy() for w in wav_row]\n",
    "        audio_html = audio_html + \"<table><tr>{}</tr></table>\".format(\n",
    "            \"\".join([f\"<td><b>{title}</b><br>{ipd.Audio(audio, rate = sr, normalize = True)._repr_html_()}</td>\" \n",
    "            for audio, title in zip(wav_row, title_row)])\n",
    "        )\n",
    "    # 音声データとタイトルを横に並べて表示\n",
    "    display(ipd.HTML(audio_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b845ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:18:54.809216Z",
     "start_time": "2024-07-14T12:18:53.640543Z"
    }
   },
   "outputs": [],
   "source": [
    "source_link = '../wavs/p225_003.wav'\n",
    "target_link = '../wavs/p227_001.wav'\n",
    "\n",
    "# VC を実行。ピッチを変換先スタイルに合わせる\n",
    "wav_vc, w24_source, sr_source, w24_target, sr_target = VC(\n",
    "    source_link, \n",
    "    target_link, \n",
    "    reconst = False, \n",
    "    pred_prosody = True, \n",
    "    return_originals = True, \n",
    ")\n",
    "\n",
    "# VC を実行。ピッチを変換先スタイルに合わせる\n",
    "wav_vc_reverse = VC(\n",
    "    target_link, \n",
    "    source_link, \n",
    "    reconst = False, \n",
    "    pred_prosody = True, \n",
    "    return_originals = False, \n",
    ")\n",
    "\n",
    "recon_source = VC(\n",
    "    source_link, \n",
    "    reconst = True, \n",
    "    pred_prosody = False, \n",
    "    return_originals = False, \n",
    ")\n",
    "\n",
    "recon_target = VC(\n",
    "    target_link, \n",
    "    reconst = True, \n",
    "    pred_prosody = False, \n",
    "    return_originals = False, \n",
    ")\n",
    "\n",
    "# 左上：ソース音声の GT、右下：ターゲット音声の GT\n",
    "\n",
    "display_audio_grid(\n",
    "    [\n",
    "        [w24_source, wav_vc_reverse],\n",
    "        [recon_source, recon_target],\n",
    "        [wav_vc, w24_target],\n",
    "    ],\n",
    "    [\n",
    "        [\"225\", \"227 to 225\"],\n",
    "        [\"225 recon\", \"227 recon\"],\n",
    "        [\"225 to 227\", \"227\"],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4c7a8",
   "metadata": {},
   "source": [
    "\n",
    "ここで投入したのはいずれも VCTK コーパスから抜粋した音声クリップであり、\n",
    "元のモデルの訓練には用いられていないデータである。\n",
    "\n",
    "喋っていない部分も微妙に声っぽいものが入ってしまうが、概ね未知話者間での声質変換（zero-shot voice conversion）が可能なことが分かるだろう。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c74d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
