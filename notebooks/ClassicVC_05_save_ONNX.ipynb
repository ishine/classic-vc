{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11113f4d",
   "metadata": {},
   "source": [
    "\n",
    "# Exporting the model components to ONNX\n",
    "\n",
    "Lyodos 著\n",
    "\n",
    "Version 1.0.0 (2024-07-14)\n",
    "\n",
    "このノートブックでは、ClassicVC の PyTorch モデルを ONNX に書き出す方法を示す。\n",
    "\n",
    "----\n",
    "\n",
    "## 準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaacac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:18:37.751607Z",
     "start_time": "2024-07-14T14:18:37.747038Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "# チェックポイントやログを保存する、機械学習関連のデータを置くルートディレクトリの指定\n",
    "\n",
    "DATASET_ROOT_PATH = Path(\"/home/lyodos/study/dataset\") # このフォルダ名はユーザーの実情に合わせて書き変えること\n",
    "\n",
    "proj_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\"\n",
    "proj_path.mkdir(parents = True, exist_ok = True)\n",
    "print(\"Project directory:\", str(proj_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9161ea",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## HarmoF0 pitch tracker の ONNX 化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4fbca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:18:48.112825Z",
     "start_time": "2024-07-14T14:18:46.944562Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import sys\n",
    "sys.path.append('../') # ClassicVC のリポジトリのルートをパスに入れて、model ディレクトリを探せるようにしている\n",
    "\n",
    "from model.harmof0.pitch_tracker import BatchedPitchEnergyTracker\n",
    "\n",
    "def pred_f0_len(length):\n",
    "    return length // 160 + 1\n",
    "\n",
    "harmof0_tracker = BatchedPitchEnergyTracker(\n",
    "    checkpoint_path = \"../model/harmof0/checkpoints/mdb-stem-synth.pth\", # HarmoF0 作者による訓練済みの重みを再配布\n",
    "    fmin = 27.5, # f0 として想定する最低周波数の Hz で、ピアノの最低音の A に相当する。\n",
    "    sample_rate = 16000,\n",
    "    hop_length = 160, # f0 を推定する間隔。160/16000 = 10 ms \n",
    "    frame_len = 1024, # sliding window を切り出す長さ\n",
    "    frames_per_step = 1000, # 1 回の forward で投入する最大セグメント数\n",
    "    high_threshold = 0.8, \n",
    "    low_threshold = 0.1, \n",
    "    freq_bins_in = 88*4,\n",
    "    bins_per_octave_in = 48,\n",
    "    bins_per_octave_out = 48,\n",
    "    device = device,\n",
    "    compile = False,\n",
    "    dry_run = 10, \n",
    ")\n",
    "\n",
    "# ちなみに初期化時点でネットワークの重みは freeze 済み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dee2a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:21:09.118347Z",
     "start_time": "2024-07-14T14:21:07.382830Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ONNX形式にエクスポート\n",
    "\n",
    "torch.onnx.export(\n",
    "    harmof0_tracker, \n",
    "    wav16,\n",
    "    str(proj_path / \"harmof0.onnx\"), \n",
    "    input_names=['input'],\n",
    "    output_names=['freq_t', 'act_t', 'energy_t', 'spec'],\n",
    "    dynamic_axes = {\n",
    "        'input': {0: 'batch', 1: 'frames'},\n",
    "        'freq_t': {0: 'batch', 1: 'frames'},\n",
    "        'act_t': {0: 'batch', 1: 'frames'},\n",
    "        'energy_t': {0: 'batch', 1: 'frames'},\n",
    "        'spec': {0: 'batch', 2: 'frames'}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde8d65",
   "metadata": {},
   "source": [
    "\n",
    "書き出した ONNX ファイルは、以下のように ONNX Runtime で推論セッションを作ったり、\n",
    "他の言語からのバインディングを通じて呼び出したりできる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d2b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:26:04.847500Z",
     "start_time": "2024-07-14T14:25:50.002965Z"
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "sess = onnxruntime.InferenceSession(\n",
    "    str(proj_path / \"harmof0.onnx\"), \n",
    "    providers  =[\n",
    "        'CUDAExecutionProvider', \n",
    "        'CPUExecutionProvider',\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(len(sess.get_inputs()))\n",
    "print(len(sess.get_outputs()))\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "input_shape = sess.get_inputs()[0].shape\n",
    "input_type = sess.get_inputs()[0].type\n",
    "print(\"Input name  :\", input_name)\n",
    "print(\"Input shape :\", input_shape)\n",
    "print(\"Input type  :\", input_type)\n",
    "\n",
    "output_name = sess.get_outputs()[0].name\n",
    "output_shape = sess.get_outputs()[0].shape\n",
    "output_type = sess.get_outputs()[0].type\n",
    "print(\"Output name  :\", output_name)\n",
    "print(\"Output shape :\", output_shape)\n",
    "print(\"Output type  :\", output_type)\n",
    "\n",
    "audio_array, sr = librosa.load('../wavs/p225_003.wav', sr = 16000, mono = True)\n",
    "if audio_array.ndim == 1:\n",
    "    audio_array = audio_array[np.newaxis, :]\n",
    "print(audio_array.shape)\n",
    "\n",
    "%time freq_t, act_t, energy_t, spec = sess.run(['freq_t', 'act_t', 'energy_t', 'spec'], {\"input\": audio_array})\n",
    "%time freq_t, act_t, energy_t, spec = sess.run(['freq_t', 'act_t', 'energy_t', 'spec'], {\"input\": audio_array})\n",
    "\n",
    "print(tensor_x.shape, freq_t.shape, act_t.shape, energy_t.shape, spec.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(spec.squeeze(), origin = \"lower\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424908a",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## (Acoustic) Style Encoder の ONNX 化\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2577af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:36.508029Z",
     "start_time": "2024-07-14T14:33:36.362605Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.StyleTTS2.models import StyleEncoder\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StyleEncoderConfig:\n",
    "    dim_in: int = 304\n",
    "    style_dim: int = 128\n",
    "    max_conv_dim: int = 512\n",
    "\n",
    "style_encoder_cfg = OmegaConf.structured(StyleEncoderConfig())\n",
    "\n",
    "style_encoder = StyleEncoder(\n",
    "    dim_in = style_encoder_cfg.dim_in, # 304\n",
    "    style_dim = style_encoder_cfg.style_dim, # 128\n",
    "    max_conv_dim = style_encoder_cfg.max_conv_dim, # 512\n",
    ")\n",
    "\n",
    "# この場所に 作った重みを 置いておく\n",
    "style_dict_path = \"../weights/style_encoder.pth\"\n",
    "\n",
    "style_dict = torch.load(style_dict_path, map_location = device)\n",
    "style_encoder.load_state_dict(style_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc0d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:40.428425Z",
     "start_time": "2024-07-14T14:33:37.910969Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tensor_x = torch.rand((1, 1, 304, 676), dtype = torch.float32)\n",
    "\n",
    "# ONNX形式にエクスポート\n",
    "torch.onnx.export(\n",
    "    style_encoder, \n",
    "    tensor_x, \n",
    "    str(proj_path / \"style_encoder_304.onnx\"), \n",
    "    opset_version = 17,\n",
    "    input_names = ['input'],\n",
    "    output_names = ['output'],\n",
    "    dynamic_axes = {\n",
    "        'input': {0: 'batch', 3: 'frames'}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94508367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:35:59.416556Z",
     "start_time": "2024-07-14T14:35:46.791707Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import onnxruntime\n",
    "\n",
    "# 実は frame が 4 の倍数でないと途中で止まる\n",
    "spec_array = np.zeros((1, 1, 304, 676), dtype = np.float32)\n",
    "\n",
    "sess = onnxruntime.InferenceSession(\n",
    "    str(proj_path / \"style_encoder_304.onnx\"), \n",
    "    providers  =[\n",
    "        'CUDAExecutionProvider', \n",
    "        'CPUExecutionProvider',\n",
    "    ],\n",
    ")\n",
    "\n",
    "%time style_onnx = sess.run(['output'], {\"input\": spec_array})[0]\n",
    "%time style_onnx = sess.run(['output'], {\"input\": spec_array})[0]\n",
    "\n",
    "print(style_onnx.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23c0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c87e86",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "## ContentVec のONNX 化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab42d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:42:16.585552Z",
     "start_time": "2024-07-14T14:42:14.109674Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import HubertConfig, HubertModel\n",
    "\n",
    "CE = HubertModel(HubertConfig())\n",
    "\n",
    "# この位置に作った重みを置いておく\n",
    "contentvec_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"contentvec_500_hubert.pth\"\n",
    "\n",
    "CE_dict = torch.load(str(contentvec_path), map_location = torch.device('cpu'))\n",
    "CE.load_state_dict(CE_dict, strict = True)\n",
    "CE.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa0312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:44:11.177181Z",
     "start_time": "2024-07-14T14:44:08.694401Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tensor_x = torch.rand((1, 16000*4), dtype = torch.float32) * 0.2\n",
    "\n",
    "onnx_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"hubert500.onnx\"\n",
    "\n",
    "# ONNX形式にエクスポート\n",
    "torch.onnx.export(\n",
    "    CE, \n",
    "    tensor_x, \n",
    "    str(proj_path / \"hubert500.onnx\"), \n",
    "    opset_version = 17,\n",
    "    input_names=['input'],\n",
    "    output_names=['last_hidden_state'],\n",
    "    dynamic_axes = {\n",
    "        'input': {0: 'batch', 1: 'samples'},\n",
    "        'last_hidden_state': {0: 'batch', 1: 'frames'}\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7b8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4cea655",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## ProsodyPredictor の ONNX 化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8e4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:48:06.513428Z",
     "start_time": "2024-07-14T14:48:06.367709Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.StyleTTS2.models import F0NPredictorAll\n",
    "\n",
    "@dataclass\n",
    "class PrododyPredictorConfig:\n",
    "    style_dim: int = 128\n",
    "    hidden_dim: int = 768\n",
    "    n_layer: int = 3\n",
    "    dropout: float = 0.2\n",
    "\n",
    "prosody_predictor_cfg = OmegaConf.structured(PrododyPredictorConfig())\n",
    "\n",
    "f0n_predictor = F0NPredictorAll(\n",
    "    style_dim = prosody_predictor_cfg.style_dim,\n",
    "    d_hid = prosody_predictor_cfg.hidden_dim,\n",
    "    nlayers = prosody_predictor_cfg.n_layer,\n",
    "    dropout = prosody_predictor_cfg.dropout,\n",
    ")\n",
    "\n",
    "\n",
    "f0n_dict_path = \"../weights/f0n_predictor.pth\"\n",
    "f0n_dict = torch.load(f0n_dict_path, map_location = \"cpu\")\n",
    "f0n_predictor.load_state_dict(f0n_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23834883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:48:37.643269Z",
     "start_time": "2024-07-14T14:48:36.599611Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "content_tensor = torch.rand((1, 768, 270), dtype = torch.float32)\n",
    "style_tensor = torch.rand((1, 128), dtype = torch.float32)\n",
    "\n",
    "# ONNX形式にエクスポート\n",
    "torch.onnx.export(\n",
    "    f0n_predictor, \n",
    "    (content_tensor, style_tensor), \n",
    "    str(proj_path / \"f0n_predictor_hubert500.onnx\"), \n",
    "    opset_version = 17, # 17 だと動かない。\n",
    "    input_names = ['content', 'style'],\n",
    "    output_names = ['pred_F0', 'pred_N'],\n",
    "    dynamic_axes = {\n",
    "        'content': {0: 'batch', 2: 'frames'},\n",
    "        'style': {0: 'batch'},\n",
    "        'pred_F0': {0: 'batch', 1: 'frames_double'},\n",
    "        'pred_N': {0: 'batch', 1: 'frames_double'},\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad3a8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# VC Decoder の ONNX 化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce9c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:50:38.086637Z",
     "start_time": "2024-07-14T14:50:37.640212Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "import math\n",
    "\n",
    "from model.StyleTTS2.hifigan import Decoder\n",
    "\n",
    "upsample_rate_list = [10, 4, 3, 2]\n",
    "\n",
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    sampling_rate: int = 24000\n",
    "    dim_in: int = 768\n",
    "    style_dim: int = 128\n",
    "    upsample_rate_list: list = tuple(upsample_rate_list)\n",
    "    upsample_kernel_list: list = tuple([i*2 for i in upsample_rate_list])\n",
    "    upsample_total: int = math.prod(upsample_rate_list)*2\n",
    "    upsample_initial_channel: int = 512\n",
    "    harmonic_num: int = 8\n",
    "\n",
    "decoder_cfg = OmegaConf.structured(DecoderConfig())\n",
    "\n",
    "decoder = Decoder(\n",
    "    sampling_rate = decoder_cfg.sampling_rate,\n",
    "    dim_in = decoder_cfg.dim_in,\n",
    "    style_dim = decoder_cfg.style_dim,\n",
    "    resblock_kernel_sizes = [3, 7, 11], # ここは大多数のモデルで同じ設定値を採用している\n",
    "    resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], # ここは大多数のモデルで同じ設定値を採用している\n",
    "    upsample_rates = decoder_cfg.upsample_rate_list,\n",
    "    upsample_initial_channel = decoder_cfg.upsample_initial_channel,\n",
    "    upsample_kernel_sizes = decoder_cfg.upsample_kernel_list,\n",
    "    harmonic_num = decoder_cfg.harmonic_num,\n",
    ")\n",
    "\n",
    "decoder_dict_path = \"../weights/decoder.pth\"\n",
    "\n",
    "decoder_dict = torch.load(decoder_dict_path, map_location = \"cpu\")\n",
    "decoder.load_state_dict(decoder_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85860da5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:51:40.507119Z",
     "start_time": "2024-07-14T14:51:27.342430Z"
    }
   },
   "outputs": [],
   "source": [
    "content_tensor = torch.rand((1, 768, 270), dtype = torch.float32)\n",
    "pred_F0 = torch.rand((1, 540), dtype = torch.float32)\n",
    "pred_N  = torch.rand((1, 540), dtype = torch.float32)\n",
    "style_tensor = torch.rand((1, 128), dtype = torch.float32)\n",
    "\n",
    "# ONNX形式にエクスポート\n",
    "torch.onnx.export(\n",
    "    decoder, \n",
    "    (content_tensor, pred_F0, pred_N, style_tensor), \n",
    "    str(proj_path / \"decoder_24k.onnx\"), \n",
    "    opset_version = 17,\n",
    "    input_names = ['content', 'pitch', 'energy', 'style'],\n",
    "    output_names = ['output'],\n",
    "    dynamic_axes = {\n",
    "        'content': {0: 'batch', 2: 'frames'},\n",
    "        'pitch': {0: 'batch', 1: 'frames_double'},\n",
    "        'energy': {0: 'batch', 1: 'frames_double'},\n",
    "        'style': {0: 'batch'},\n",
    "    }\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
