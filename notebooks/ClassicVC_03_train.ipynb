{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11113f4d",
   "metadata": {},
   "source": [
    "\n",
    "# Training ClassicVC\n",
    "\n",
    "Lyodos 著\n",
    "\n",
    "Version 1.0.0 (2024-07-14)\n",
    "\n",
    "前提は、前のノートブックを実行してデータセットの音声をダウンロード＆フォーマット変換し、メタデータを作成済みであること。\n",
    "\n",
    "まず、チェックポイントやログ等を書き出すためのフォルダを設定する。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaacac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:19.669752Z",
     "start_time": "2024-07-14T13:42:19.664455Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "# チェックポイントやログを保存する、プロジェクト単位および訓練ジョブ単位のルートディレクトリの指定\n",
    "\n",
    "DATASET_ROOT_PATH = Path(\"/home/lyodos/study/dataset\") # このフォルダ名はユーザーの実情に合わせて書き変えること\n",
    "\n",
    "# まず、プロジェクト（ClassicVC）全体で保存先を決める。ちなみに使うデータセットの組み合わせでサブディレクトリを作っている。\n",
    "proj_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"voxceleb12_libri_samr\"\n",
    "proj_path.mkdir(parents = True, exist_ok = True)\n",
    "print(\"Project directory:\", str(proj_path))\n",
    "\n",
    "# 訓練ジョブに名前を付ける\n",
    "JOB_NAME = \"VC01\"\n",
    "\n",
    "# チェックポイントとログの保存先を定義する。proj_path の下にジョブ名でフォルダを作る。\n",
    "ckpt_path = Path(proj_path) / JOB_NAME\n",
    "ckpt_path.mkdir(parents = True, exist_ok = True)\n",
    "print(\"Training job directory:\", str(ckpt_path))\n",
    "\n",
    "logs_path = ckpt_path / \"tensorboard\" # TensorBoard 用のログは数が多くなるので、ジョブ名の下にサブフォルダを作ってまとめる。\n",
    "logs_path.mkdir(parents = True, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa01145",
   "metadata": {},
   "source": [
    "\n",
    "次に logger （TensorBoard ではなく Notebook 内の情報を確認するためのもの）を作成する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332be44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:19.674374Z",
     "start_time": "2024-07-14T13:42:19.671072Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) #出力レベルの設定\n",
    "\n",
    "for h in logger.handlers[:]:\n",
    "    logger.removeHandler(h) # logger を新しく作成した場合、既存の handler があれば全て除去する。\n",
    "\n",
    "file_handler = logging.FileHandler(ckpt_path / 'classic-vc.log') # ログファイル用ハンドラの生成\n",
    "sout_handler = logging.StreamHandler() # 標準出力用ハンドラの生成\n",
    "\n",
    "fmt = logging.Formatter('%(asctime)s %(message)s') # フォーマッタの生成\n",
    "file_handler.setFormatter(fmt) # ハンドラにフォーマッタを登録\n",
    "sout_handler.setFormatter(fmt) # ハンドラにフォーマッタを登録\n",
    "\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(file_handler) # ロガーにハンドラを登録\n",
    "    logger.addHandler(sout_handler)\n",
    "\n",
    "logger.info('Checkpoints are saved to: {}'.format(str(ckpt_path)))\n",
    "logger.info('TensorBoard log is saved to : {}'.format(str(logs_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08053519",
   "metadata": {},
   "source": [
    "\n",
    "訓練に使う GPU を指定する。現在マルチ GPU には対応していない。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272a13d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:20.464286Z",
     "start_time": "2024-07-14T13:42:19.675437Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    device = torch.device('cuda:0')\n",
    "elif torch.cuda.device_count() >= 1:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "logger.info('PyTorch device: {}'.format(device))\n",
    "logger.info('PyTorch device name: {}'.format(torch.cuda.get_device_name(device = device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b504eae",
   "metadata": {},
   "source": [
    "\n",
    "以下、ネットワークの部品を定義していく。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9161ea",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## HarmoF0 pitch tracker の定義\n",
    "\n",
    "\n",
    "* 入力は `torch.Size([n_batch, n_sample])` ただし `(1, 513)` （32.1 ms）以上の wave length がないとエラーになる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05b7b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:21.496401Z",
     "start_time": "2024-07-14T13:42:20.465543Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import sys\n",
    "sys.path.append('../') # ClassicVC のリポジトリのルートをパスに入れて、model ディレクトリを探せるようにしている\n",
    "\n",
    "from model.harmof0.pitch_tracker import BatchedPitchEnergyTracker\n",
    "\n",
    "def pred_f0_len(length):\n",
    "    return length // 160 + 1\n",
    "\n",
    "harmof0_tracker = BatchedPitchEnergyTracker(\n",
    "    checkpoint_path = \"../model/harmof0/checkpoints/mdb-stem-synth.pth\", # HarmoF0 作者による訓練済みの重みを再配布\n",
    "    fmin = 27.5, # f0 として想定する最低周波数の Hz で、ピアノの最低音の A に相当する。\n",
    "    sample_rate = 16000,\n",
    "    hop_length = 160, # f0 を推定する間隔。160/16000 = 10 ms \n",
    "    frame_len = 1024, # sliding window を切り出す長さ\n",
    "    frames_per_step = 1000, # 1 回の forward で投入する最大セグメント数\n",
    "    high_threshold = 0.8, \n",
    "    low_threshold = 0.1, \n",
    "    freq_bins_in = 88*4,\n",
    "    bins_per_octave_in = 48,\n",
    "    bins_per_octave_out = 48,\n",
    "    device = device,\n",
    "    compile = False,\n",
    "    dry_run = 10, \n",
    ")\n",
    "\n",
    "num_harmof0_params = sum([p.numel() for p in list(harmof0_tracker.single_tracker.net.parameters())])\n",
    "logger.info(f\"Pitch tracker has {num_harmof0_params} parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424908a",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## (Acoustic) Style Encoder の初期化\n",
    "\n",
    "$E_{a}(x)$ を作成する。\n",
    "入力はスペクトログラムで、さらに `(batch, 1, dim_spec, n_frame >= 80)` の 4D テンソルでないと受けられない。\n",
    "\n",
    "なお HarmoF0 の低音側は 27.5 Hz だが、下の 1 オクターブ（48 bins）は人の声で使うことはなく無駄なので、\n",
    "入力は 352 ではなく 352 - 48 = 304 にする。これでパラメータ数を 23,719,264 → 23,189,104  に削減できる。\n",
    "\n",
    "出力は時間次元を持たない `(batch, 128)`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2577af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:21.633940Z",
     "start_time": "2024-07-14T13:42:21.498769Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.StyleTTS2.models import StyleEncoder\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StyleEncoderConfig:\n",
    "    dim_in: int = 304\n",
    "    style_dim: int = 128\n",
    "    max_conv_dim: int = 512\n",
    "\n",
    "style_encoder_cfg = OmegaConf.structured(StyleEncoderConfig())\n",
    "with open(ckpt_path / \"style_encoder_cfg.yaml\", 'w') as handle:\n",
    "    OmegaConf.save(config = style_encoder_cfg, f = handle)\n",
    "\n",
    "style_encoder = StyleEncoder(\n",
    "    dim_in = style_encoder_cfg.dim_in, # 304\n",
    "    style_dim = style_encoder_cfg.style_dim, # 128\n",
    "    max_conv_dim = style_encoder_cfg.max_conv_dim, # 512\n",
    ").to(device)\n",
    "\n",
    "\n",
    "num_style_encoder_params = sum([p.numel() for p in list(style_encoder.parameters())])\n",
    "logger.info(f\"Pitch tracker has {num_style_encoder_params} parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c87e86",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Content encoder (ContentVec)\n",
    "\n",
    "\n",
    "Content の抽出に使う ContentVec のネットワーク構造は、transformers パッケージの HubertModel を使う。\n",
    "\n",
    "> 重みは ContentVec の公式で配布しているものを若干アレンジする必要があるので、Notebook 04 に書いてある手順を参照して用意すること。\n",
    "\n",
    "\n",
    "* 入力は `torch.Size([n_batch, n_sample])` の 16 khz mono waveform だが、 `(1, 400)` （25 ms）以上ないとエラーになる。\n",
    "\n",
    "* HubertModel の生出力は feature last であるが、ほとんどの下流工程は time last になるので注意。\n",
    "\n",
    "* 出力サイズは `torch.Size([batch, ((length - 80) // 320), 768])` で定義される。\n",
    "\n",
    "入力が 16000 Hz で 1/320 に間引くので **ContentVec の出力テンソルの hop は 20 ms である**。\n",
    "\n",
    "* HarmoF0 pitch tracker は 10 ms hop なので、ちょうど端数以外は 2 倍のテンソルサイズとなる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c695b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:25.140250Z",
     "start_time": "2024-07-14T13:42:21.634860Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import HubertConfig, HubertModel\n",
    "\n",
    "CE = HubertModel(HubertConfig())\n",
    "\n",
    "# この位置に作った重みを置いておく\n",
    "contentvec_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"contentvec_500_hubert.pth\"\n",
    "\n",
    "CE_dict = torch.load(str(contentvec_path), map_location = torch.device('cpu'))\n",
    "CE.load_state_dict(CE_dict, strict = True)\n",
    "CE.eval().to(device)\n",
    "\n",
    "num_CE_params = sum([p.numel() for p in list(CE.parameters())])\n",
    "logger.info(f\"ContentVec has {num_CE_params} parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cea655",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## ProsodyPredictor の定義\n",
    "\n",
    "ContentVec の抽出した特徴量と、Style Encoder で抽出した話者スタイル $s_a$ をもとに、\n",
    "F0 と energy の時間変化を予測するネットワークである。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8e4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:25.299805Z",
     "start_time": "2024-07-14T13:42:25.141381Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from model.StyleTTS2.models import F0NPredictorAll\n",
    "\n",
    "@dataclass\n",
    "class PrododyPredictorConfig:\n",
    "    style_dim: int = 128\n",
    "    hidden_dim: int = 768\n",
    "    n_layer: int = 3\n",
    "    dropout: float = 0.2\n",
    "\n",
    "prosody_predictor_cfg = OmegaConf.structured(PrododyPredictorConfig())\n",
    "\n",
    "with open(ckpt_path / \"prosody_predictor_cfg.yaml\", 'w') as handle:\n",
    "    OmegaConf.save(config = prosody_predictor_cfg, f = handle)\n",
    "\n",
    "# 新造した F0NPredictorAll は time last で入れる。返り値はそれぞれ channel 次元が潰れた 2D の time last だが、時間解像度が 2 倍\n",
    "f0n_predictor = F0NPredictorAll(\n",
    "    style_dim = prosody_predictor_cfg.style_dim,\n",
    "    d_hid = prosody_predictor_cfg.hidden_dim,\n",
    "    nlayers = prosody_predictor_cfg.n_layer,\n",
    "    dropout = prosody_predictor_cfg.dropout,\n",
    ").to(device)\n",
    "\n",
    "num_f0n_predictor_params = sum([p.numel() for p in list(f0n_predictor.parameters())])\n",
    "logger.info(f\"Prosody predictor has {num_f0n_predictor_params} parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad3a8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# VC Decoder の定義\n",
    "\n",
    "StyleTTS 2 には iSTFTNet ベースのデコーダもあるが、スピードと音質を考えて、HiFi-GAN ベースだけをとりあえず考える。\n",
    "\n",
    "```python\n",
    "y_rec = decoder(\n",
    "    en, # content 情報 torch.Size([1, 512, 445]) # 最終次元が長さ依存\n",
    "    F0_real, # F0 torch.Size([1, 890]) # content 特徴量の長さの 2 倍\n",
    "    real_norm, # Energy torch.Size([1, 890]) # content 特徴量の長さの 2 倍\n",
    "    s, # acoustic style は固定サイズ torch.Size([1, 128])\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce9c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:25.678833Z",
     "start_time": "2024-07-14T13:42:25.300766Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "import math\n",
    "\n",
    "from model.StyleTTS2.hifigan import Decoder\n",
    "\n",
    "upsample_rate_list = [10, 4, 3, 2]\n",
    "\n",
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    sampling_rate: int = 24000\n",
    "    dim_in: int = 768\n",
    "    style_dim: int = 128\n",
    "    upsample_rate_list: list = tuple(upsample_rate_list)\n",
    "    upsample_kernel_list: list = tuple([i*2 for i in upsample_rate_list])\n",
    "    upsample_total: int = math.prod(upsample_rate_list)*2\n",
    "    upsample_initial_channel: int = 512\n",
    "    harmonic_num: int = 8\n",
    "\n",
    "decoder_cfg = OmegaConf.structured(DecoderConfig())\n",
    "\n",
    "with open(ckpt_path / \"decoder_cfg.yaml\", 'w') as handle:\n",
    "    OmegaConf.save(config = decoder_cfg, f = handle)\n",
    "\n",
    "decoder = Decoder(\n",
    "    sampling_rate = decoder_cfg.sampling_rate,\n",
    "    dim_in = decoder_cfg.dim_in,\n",
    "    style_dim = decoder_cfg.style_dim,\n",
    "    resblock_kernel_sizes = [3, 7, 11], # ここは大多数のモデルで同じ設定値を採用している\n",
    "    resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]], # ここは大多数のモデルで同じ設定値を採用している\n",
    "    upsample_rates = decoder_cfg.upsample_rate_list,\n",
    "    upsample_initial_channel = decoder_cfg.upsample_initial_channel,\n",
    "    upsample_kernel_sizes = decoder_cfg.upsample_kernel_list,\n",
    "    harmonic_num = decoder_cfg.harmonic_num,\n",
    ").to(device)\n",
    "\n",
    "num_dec_params = sum([p.numel() for p in list(decoder.parameters())])\n",
    "logger.info(f\"Decoder has {num_dec_params} parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1367da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T13:51:33.905049Z",
     "start_time": "2024-03-17T13:51:33.902675Z"
    }
   },
   "source": [
    "\n",
    "実は注意が必要で、\n",
    "`F0_real`, `real_norm` は運用時は偶数のフレーム長でなければならない。\n",
    "さもないと content が // 2 で短くなってしまうため、割り切れない長さになり、decoder 内の AdainResBlk1d でエラーが出る。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63d2bf",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "\n",
    "# VC 訓練用のデータセット\n",
    "\n",
    "Notebook 02 で作成して保存したメタデータを、proj_path に置く。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c112c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:31.331557Z",
     "start_time": "2024-07-14T13:42:25.681034Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import mgzip\n",
    "import pickle\n",
    "\n",
    "meta_name = str(proj_path / \"classic-vc-meta.pkl.gz\")\n",
    "\n",
    "with mgzip.open(meta_name, \"rb\") as f:\n",
    "    metadata = pickle.load(f) # 訓練ルートフォルダに配置済みの、話者＆発話データ一覧をロード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d4b920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:31.355974Z",
     "start_time": "2024-07-14T13:42:31.332744Z"
    }
   },
   "outputs": [],
   "source": [
    "from rich.pretty import pprint \n",
    "\n",
    "print(len(metadata))\n",
    "print(len(metadata[0]))\n",
    "pprint(metadata[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12978952",
   "metadata": {},
   "source": [
    "\n",
    "詳細な訓練設定を作る。\n",
    "なお learning rate は中途半端な下がり方だが、これはレガシーなコードが残っているだけなので、\n",
    "たぶんスケジューラ自体を削除して固定値で訓練しても上手く行く。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b907b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:31.490217Z",
     "start_time": "2024-07-14T13:42:31.357049Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from omegaconf import OmegaConf\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.dataset import VCDataset, WavCollator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VCTrainConfig:\n",
    "    seed: int = 42\n",
    "    val_speakers: int = 128 # 総データセットのうち何話者分を valid に振り分けるか\n",
    "    batch_size: int = 2 # train データセットを、1 回につき何話者ずつ振り出すか\n",
    "    val_batch_size: int = 8 # val データセットを、1 回につき何話者ずつ振り出すか\n",
    "    n_utterances: int = 1 # train/val データセットを、1 回 1 話者につき何発話ずつ振り出すか。\n",
    "    sec: float = 3.0 # 1st は 3 秒だった。サンプル秒数を一意に定める場合はここに秒数（例： 2.0）を指定。None なら max_sec に合わせる\n",
    "    min_sec: float = 2.0 # 教師データの wav が最低限満たすべき有効な秒数\n",
    "    max_sec: float = 5.0 # sec = None の場合に最長限度とするサンプル秒数 = valid dataset の場合\n",
    "    sr: int = decoder_cfg.sampling_rate # 訓練中に取り扱う最高精度のサンプリング周波数\n",
    "\n",
    "    n_workers: int = 4 # データローダーに使うプロセス立ち上げ数。多すぎると RAM が尽きる\n",
    "    fp16: bool = False # 半精度による高速化 → やたらと nan が出まくるので使わないことにした（ただし bf16 なら行けるっぽい）\n",
    "    board_every: int = 50 # Tensorboard への反映。単位 steps\n",
    "    valid_every: int = 1 # validation set による評価。単位 epochs\n",
    "    save_every: int = 5 # 保存間隔。単位 epochs\n",
    "\n",
    "    start_lr: float = 1e-4 # Learning settings\n",
    "    max_lr: float = 1e-4 # 3.5e-5 を超えたあたりから val loss が暴れる。\n",
    "    end_lr: float = 1e-5 # end は 3e-7 で試していたが、もっと大きくてもいいだろう。\n",
    "    n_warmup_epochs: int = 10\n",
    "    t_initial: int = 3000 # スケジューラの 1 サイクルのエポック数\n",
    "    n_epochs: int = 1000 # 最大エポック数\n",
    "    TMA_epoch: int = 5 # 1st stage training において最初の n epochs は TMA を行わず spec loss だけを計算する。デフォルト 5\n",
    "    cycle_decay: float = 1.0 # 再起動ごとに max_lr の値に倍数をかける。100万ステップ（5 回）で 1e-5 まで落としたい\n",
    "    k_decay: float = 1.0 # CosineLR の低下速度。1.0 が標準で、 0 < k_decay < 1 だと速く落ちる。\n",
    "    cycle_limit: int = 10 # 上限エポックまでに、何回のサイクルを回すか。1 なら再起動なし\n",
    "    betas: typing.Tuple[float, float] = (0.8, 0.99) # HiFi-GAN に合わせた\n",
    "    grad_clip: float = 10.0\n",
    "    f0_act_threshold: float = 0.7 # HarmoF0 の activation に基づき、有声部のみを損失計算に使う\n",
    "    lambda_spec: float = 5.0 # スペクトログラム再構成損失の係数\n",
    "    lambda_F0: float = 0.1 # F0 の再構成損失の係数\n",
    "    lambda_norm: float = 1.0 # Energy の再構成損失の係数\n",
    "    lambda_gen: float = 1.0 # loss_gen_all の係数 = generator loss\n",
    "    lambda_slm: float = 1.0 # loss_slm の係数 = slm feature matching loss\n",
    "    lambda_sty: float = 1.0 # loss_sty の係数\n",
    "\n",
    "tr_cfg = OmegaConf.structured(VCTrainConfig())\n",
    "\n",
    "with open(ckpt_path / \"train_cfg.yaml\", 'w') as handle:\n",
    "    OmegaConf.save(config = tr_cfg, f = handle)\n",
    "\n",
    "\n",
    "# 辞書形式の状態で train/val を分ける\n",
    "random.seed(tr_cfg.seed)\n",
    "random.shuffle(metadata)\n",
    "valid_meta = metadata[:tr_cfg.val_speakers]\n",
    "#train_meta = metadata[:tr_cfg.val_speakers] # データセットの一部分で最初にコードを検証するときに使う\n",
    "train_meta = metadata[tr_cfg.val_speakers:]\n",
    "\n",
    "trainset = VCDataset(\n",
    "    train_meta, #  全ての「話者＆発話」を一覧できる dict 形式データ\n",
    "    n_utterances = tr_cfg.n_utterances, # 1 回の呼び出しでサンプルしたい 1 話者分の発話数（データセットに実際含まれる数ではない）\n",
    "    sampling_rate = tr_cfg.sr, # ネットワークに流すための事前整形で目標とするサンプリング周波数\n",
    "    min_sec = tr_cfg.min_sec, # 教師データの wav が最低限満たすべき有効な秒数\n",
    "    sec = tr_cfg.sec, # サンプル秒数を一意に定める場合はここに秒数（例： 2.0）を指定\n",
    "    max_sec = tr_cfg.max_sec, # sec = None の場合に最長限度とするサンプル秒数\n",
    "    with_ref = True, # 同じ話者からランダムに選んだ別の発話も reference として持ってくる。バッチサイズは同じ。\n",
    "    valid_mode = False, # モードが train か valid か。\n",
    ")\n",
    "\n",
    "validset = VCDataset(\n",
    "    valid_meta,\n",
    "    n_utterances = tr_cfg.n_utterances, \n",
    "    sampling_rate = tr_cfg.sr,\n",
    "    min_sec = tr_cfg.min_sec, \n",
    "    sec = None, # None なら max_sec に合わせる\n",
    "    max_sec = tr_cfg.max_sec, # sec = None の場合に最長限度とするサンプル秒数\n",
    "    valid_mode = True, # モードが train か valid か。\n",
    ")\n",
    "\n",
    "assert len(trainset) >= tr_cfg.batch_size\n",
    "assert len(validset) >= tr_cfg.batch_size\n",
    "logger.info('Use {} speakers for training.'.format(len(trainset)))\n",
    "logger.info('Use {} speakers for validation.'.format(len(validset)))\n",
    "\n",
    "collater = WavCollator()\n",
    "\n",
    "# DataLoader は train と val がそれぞれ必要\n",
    "train_loader = DataLoader(\n",
    "    trainset, \n",
    "    num_workers = tr_cfg.n_workers, \n",
    "    shuffle = True,\n",
    "    sampler = None, # サンプリングではなく冒頭から振り出していく\n",
    "    batch_size = tr_cfg.batch_size,\n",
    "    pin_memory = False, # デフォルト False\n",
    "    drop_last = True, # batch size で区切っていったときの端数を訓練に使わない\n",
    "    collate_fn = collater,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    validset, \n",
    "    num_workers = tr_cfg.n_workers, \n",
    "    shuffle = False, # データセット内での話者順をランダム化せずに冒頭から振り出す\n",
    "    sampler = None,\n",
    "    batch_size = tr_cfg.val_batch_size,\n",
    "    pin_memory = False,\n",
    "    drop_last = True,\n",
    "    collate_fn = collater,\n",
    ")\n",
    "\n",
    "logger.info('Dataset and DataLoader are generated.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5815e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:42:31.697410Z",
     "start_time": "2024-07-14T13:42:31.491411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# データセットの最初のバッチを実際に振り出してみる\n",
    "\n",
    "print(\"\\n\", \"iter\")\n",
    "%time Iter = iter(train_loader)\n",
    "\n",
    "print(\"\\n\", \"next\")\n",
    "%time wavs, filenames, speakers, ref_wavs, ref_filenames = next(Iter)\n",
    "print(wavs.shape) \n",
    "print(ref_wavs.shape)\n",
    "\n",
    "from rich.pretty import pprint \n",
    "pprint(filenames)\n",
    "pprint(ref_filenames)\n",
    "pprint(speakers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3ca6c",
   "metadata": {},
   "source": [
    "\n",
    "損失の定義。基本的に StyleTTS 2 から来ている\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97fc11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:43:54.115923Z",
     "start_time": "2024-07-14T13:42:31.699604Z"
    }
   },
   "outputs": [],
   "source": [
    "# 損失関数、オプティマイザ、スケジューラのインスタンス化\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "\n",
    "from model.utils import scan_checkpoint\n",
    "from model.StyleTTS2.losses import MultiResolutionSTFTLoss, GeneratorLoss, DiscriminatorLoss, WavLMLoss\n",
    "from model.StyleTTS2.discriminators import MultiPeriodDiscriminator, MultiResSpecDiscriminator, WavLMDiscriminator\n",
    "\n",
    "\n",
    "mpd = MultiPeriodDiscriminator(periods = [2, 3, 5, 7, 11, 17]).to(device)\n",
    "msd = MultiResSpecDiscriminator()\n",
    "wd = WavLMDiscriminator(slm_hidden = 768, slm_layers = 13, initial_channel = 64)\n",
    "\n",
    "num_mpd_params = sum([p.numel() for p in list(mpd.parameters())])\n",
    "logger.info(f\"MultiPeriodDiscriminator has {num_mpd_params} parameters.\")\n",
    "num_msd_params = sum([p.numel() for p in list(msd.parameters())])\n",
    "logger.info(f\"MultiResSpecDiscriminator has {num_msd_params} parameters.\")\n",
    "\n",
    "# 48k 化する場合は fft の解像度を増やす必要がある。さもないと低音が評価から外れ、ピッチが変化しなくなるのでロボ声化する。\n",
    "# 実は速度にはさほど影響しないので、もっと長い fft を掛けてもいいかも\n",
    "stft_loss = MultiResolutionSTFTLoss(\n",
    "    sample_rate = decoder_cfg.sampling_rate,\n",
    "    fft_sizes = [2048, 1024, 512],\n",
    "    win_lengths = [1200, 600, 240], # 24k 信号に対して [600, 1200, 240] = 25 ms, 50 ms, 10 ms\n",
    "    hop_sizes = [240, 120, 50], # 24k 信号に対して [120, 240, 50] = 5 ms, 10 ms, 2.8 ms\n",
    ").to(device)\n",
    "\n",
    "gl = GeneratorLoss(mpd, msd).to(device)\n",
    "dl = DiscriminatorLoss(mpd, msd).to(device)\n",
    "\n",
    "# wavlm の重みをキャッシュにダウンロードするので 378 MB の通信が行われるっぽい\n",
    "wl = WavLMLoss(\n",
    "    'microsoft/wavlm-base-plus', \n",
    "    wd, \n",
    "    model_sr = tr_cfg.sr, # 24000 \n",
    "    slm_sr = 16000 # SLM は入力信号の sr にかかわらず内部で 16k に自動変換されてから処理される\n",
    ").to(device)\n",
    "\n",
    "#### ここからエポックの定義\n",
    "\n",
    "# 計算ステップと最終エポックはここで 0, -1 で初期化しておき、既存チェックポイントを読んで再開する場合は上書きする\n",
    "step = 0\n",
    "last_epoch = -1 # 最初は「前回 epoch = -1」なので、つまり今回 epoch = 0 の状態から開始\n",
    "state_dict = None # 最初は前回のチェックポイントがないものとして扱う\n",
    "\n",
    "# 学習再開のための state_dict 探索、ロード、ステップ数の調整\n",
    "\n",
    "load_weight = False # 前回のチェックポイントを探索＆ロードするか。初回の場合は False に。\n",
    "\n",
    "if load_weight:\n",
    "    logging.info(f\"Scanning checkpoints directory : {str(ckpt_path)}\")\n",
    "    LAST_CKPT_NAME = scan_checkpoint(str(ckpt_path), prefix = \"vc_1st_\") # ステップ数が大きい pt を自動探索\n",
    "    if LAST_CKPT_NAME is not None:\n",
    "        logger.info(f\"Loading the last checkpoint {LAST_CKPT_NAME} ...\")\n",
    "        state_dict = torch.load(LAST_CKPT_NAME, map_location = device)\n",
    "        style_encoder.load_state_dict(state_dict['style_encoder'])\n",
    "        f0n_predictor.load_state_dict(state_dict['f0n_predictor'])\n",
    "        decoder.load_state_dict(state_dict['decoder'])\n",
    "        mpd.load_state_dict(state_dict['mpd'])\n",
    "        msd.load_state_dict(state_dict['msd'])\n",
    "        wd.load_state_dict(state_dict['wd'])\n",
    "        step = state_dict['step'] + 1 # 前回最終状態に 1 足したステップから訓練開始。\n",
    "        last_epoch = state_dict['epoch']\n",
    "    else:\n",
    "        logger.info(f\"No checkpoints are found under {str(ckpt_path)}\")\n",
    "\n",
    "\n",
    "# オプティマイザをインスタンス化。GAN では g, do それぞれについて作成する（do は MPD と MSD の両方のパラメータを制御する）\n",
    "# 学習可能なパラメータがない、もしくは重みを固定して使うネットワークは含めない\n",
    "# 定義には、チューニング対象（通常は model オブジェクト）に存在するパラメータオブジェクトと、学習率を与える。\n",
    "\n",
    "optimizer_g = AdamW(\n",
    "    params = chain(\n",
    "        style_encoder.parameters(), \n",
    "        f0n_predictor.parameters(), \n",
    "        decoder.parameters(),\n",
    "    ), # 全層を対象にする場合\n",
    "    lr = tr_cfg.max_lr, # 基本学習率\n",
    "    betas = tr_cfg.betas,\n",
    ")\n",
    "\n",
    "optimizer_do = AdamW(\n",
    "    params = chain(\n",
    "        mpd.parameters(), \n",
    "        msd.parameters(), \n",
    "    ), # 全層を対象にする場合\n",
    "    lr = tr_cfg.max_lr, # 基本学習率\n",
    "    betas = tr_cfg.betas,\n",
    ")\n",
    "\n",
    "# オプティマイザの保存済みパラメータもロードする\n",
    "if state_dict is not None:\n",
    "    optimizer_g.load_state_dict(state_dict['optim_g'])\n",
    "    optimizer_do.load_state_dict(state_dict['optim_do'])\n",
    "\n",
    "####\n",
    "\n",
    "# 続いてスケジューラを作成。継続の場合、オプティマイザの state_dict がロードされていないと正常に動かない。\n",
    "\n",
    "scheduler_g = CosineLRScheduler(\n",
    "    optimizer_g, \n",
    "    t_initial = tr_cfg.t_initial, \n",
    "    lr_min = tr_cfg.end_lr, \n",
    "    warmup_t = tr_cfg.n_warmup_epochs, \n",
    "    warmup_lr_init = tr_cfg.start_lr, \n",
    "    warmup_prefix = True, # Warmupが完了したタイミングの学習率を、オプティマイザーの基本学習率の設定値に合わせる\n",
    "    cycle_decay = tr_cfg.cycle_decay, \n",
    "    k_decay = tr_cfg.k_decay,\n",
    "    cycle_limit = tr_cfg.cycle_limit, # 1 だと SGDR の再起動を行わない\n",
    ")\n",
    "\n",
    "scheduler_do = CosineLRScheduler(\n",
    "    optimizer_do, \n",
    "    t_initial = tr_cfg.t_initial, \n",
    "    lr_min = tr_cfg.end_lr, \n",
    "    warmup_t = tr_cfg.n_warmup_epochs, \n",
    "    warmup_lr_init = tr_cfg.start_lr, \n",
    "    warmup_prefix = True,\n",
    "    cycle_decay = tr_cfg.cycle_decay, \n",
    "    k_decay = tr_cfg.k_decay,\n",
    "    cycle_limit = tr_cfg.cycle_limit,\n",
    ")\n",
    "\n",
    "if state_dict is not None:\n",
    "    scheduler_g.load_state_dict(state_dict['schedule_g'])\n",
    "    scheduler_do.load_state_dict(state_dict['schedule_do'])\n",
    "    logger.info(f\"Resume training from epoch {last_epoch}, step {step}\")\n",
    "else:\n",
    "    logger.info(f\"Start training from epoch {last_epoch + 1}, step {step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6177a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:43:54.617373Z",
     "start_time": "2024-07-14T13:43:54.116895Z"
    }
   },
   "outputs": [],
   "source": [
    "# スケジューラのプロット（model 用の utils で matplotlib.use(\"Agg\") を使う場合はここより下で定義すること）\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "lrs = []\n",
    "for t in range(tr_cfg.n_epochs):\n",
    "    lrs.append(scheduler_g._get_lr(t*1))\n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.semilogy()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0e497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:43:56.824974Z",
     "start_time": "2024-07-14T13:43:54.618630Z"
    }
   },
   "outputs": [],
   "source": [
    "# すでに走っていますと言われる場合、Reusing TensorBoard on port 6006 (pid 506885), started 4:34:13 ago. (Use '!kill 506885' to kill it.)\n",
    "!rm -rf /tmp/.tensorboard-info/\n",
    "\n",
    "%load_ext tensorboard\n",
    "#reload_ext tensorboard\n",
    "LOG_DIR = str(logs_path)\n",
    "%tensorboard --logdir $LOG_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a76af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T15:01:13.074996Z",
     "start_time": "2024-07-14T13:43:56.826627Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio\n",
    "\n",
    "from model.utils import plot_spectrogram_harmof0\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "resample_orig_to16 = torchaudio.transforms.Resample(orig_freq = tr_cfg.sr, new_freq = 16000).to(device)\n",
    "resample_orig_to24 = torchaudio.transforms.Resample(orig_freq = tr_cfg.sr, new_freq = 24000)\n",
    "\n",
    "writer = SummaryWriter(logs_path) # TensorBoard 用のイベントファイルを吐くオブジェクト\n",
    "\n",
    "path_remote_log = os.path.join(\".\", \"train_output.txt\") # 損失値だけチラ見するためのログファイル\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() # 訓練開始時に一度、GradScaler をインスタンス化しておく\n",
    "\n",
    "first_step = True # Ground truth を TensorBoard に書き出すために、最初の step のみ True とするフラグ\n",
    "\n",
    "# 訓練時のプログレスバーを定義\n",
    "mb = master_bar(range(max(0, last_epoch), tr_cfg.n_epochs)) # last_epoch は 最初は -1 なので、0 との max を取る必要がある。\n",
    "total_start = time.time()\n",
    "\n",
    "# epoch が「現在のエポック」を保持する変数。last_epoch は以下のイテレーション内では再代入されることはない。\n",
    "for epoch in mb:\n",
    "    # エポックの開始。epoch 変数は 0 始まりなので表示は + 1\n",
    "    start = time.time()\n",
    "    mb.main_bar.comment = ' Epoch: {:,d} / {:,d} (peak memory = {:5.2f} GB)'.\\\n",
    "            format(epoch + 1, tr_cfg.n_epochs, torch.cuda.max_memory_allocated()/1e9)\n",
    "    pb = progress_bar(enumerate(train_loader), total = len(train_loader), parent = mb)\n",
    "\n",
    "    #### Validation は valid_every エポックごとに実行する仕様に変更した\n",
    "    if epoch % tr_cfg.valid_every == 0:\n",
    "        style_encoder.eval() # 一時的に推論モードに\n",
    "        f0n_predictor.eval()\n",
    "        decoder.eval()\n",
    "        val_loss_spec = 0.0\n",
    "        val_loss_F0 = 0.0\n",
    "        val_loss_norm = 0.0\n",
    "        val_loss_content = 0.0\n",
    "        val_loss_speaker = 0.0\n",
    "        with torch.no_grad():\n",
    "            for j, batch in enumerate(valid_loader):\n",
    "                wav_orig = batch[0]\n",
    "                filenames = batch[1] # [\"path\", \"path\", \"path\", \"path\"]\n",
    "                speakers = batch[2] # [\"name\", \"name\", \"name\", \"name\"]\n",
    "                # Valid dataset の順伝播。最初に 16k 化してHarmoF0 に通す\n",
    "                w16 = resample_orig_to16(wav_orig.to(device))\n",
    "                F0_real, act_t, N_real, spec = harmof0_tracker(w16)\n",
    "                # pitch, energy の従来手法による推定値を得た後は、(acoustic) style encoder\n",
    "                s_a = style_encoder(spec[:, 48:, :].unsqueeze(1)) # 4D にして投入\n",
    "                # 次に ContentVec\n",
    "                content = CE(w16)[\"last_hidden_state\"].transpose(2, 1) # time last に転置\n",
    "                # content が計算できたら、pitch と energy の予測値を作る\n",
    "                F0_fake, N_fake = f0n_predictor(\n",
    "                    content, # (batch, d_model = 768, n_frame) の time last\n",
    "                    s_a, # =  style embedding, torch.Size([1, 128])\n",
    "                )\n",
    "                reconst = decoder(\n",
    "                    content, # (batch, 768, frame)\n",
    "                    F0_fake[:, -content.size(2)*2:], # (batch, frame*2) \n",
    "                    N_fake[:, -content.size(2)*2:],  # (batch, frame*2)\n",
    "                    s_a, # (batch, 128)\n",
    "                ) # 再構成した音声は singleton dimension が入った (batch, 1, time*) \n",
    "                # なお再構成側が元音声よりも 1 フレーム分短くなる。これは無視する。\n",
    "\n",
    "                # 損失の計算。Valid は spectrogram の再構成損失だけを計算\n",
    "                val_loss_spec += stft_loss(reconst.squeeze(), wav_orig[:, -reconst.shape[-1]:].detach().to(device))\n",
    "                avg_val_loss = val_loss_spec / (j + 1) # val エラー率の平均値\n",
    "                # 次に F0 と N の損失だが、実は real の方が 3 フレーム長い\n",
    "                # リアルタイム VC だと基本的に末尾（時間的に新しい）を採用するので、こちらもそれに合わせる。\n",
    "                val_loss_F0 +=  F.smooth_l1_loss(F0_real[:, -content.size(2)*2:], F0_fake[:, -content.size(2)*2:]) # F0 予測値の差\n",
    "                avg_val_F0 = val_loss_F0 / (j + 1)\n",
    "                val_loss_norm += F.smooth_l1_loss(N_real[:, -content.size(2)*2:], N_fake[:, -content.size(2)*2:]) # Energy 予測値の差\n",
    "                avg_val_norm = val_loss_norm / (j + 1)\n",
    "                mb.child.comment = f'Validating (epoch = {epoch:,d}, val loss = {avg_val_loss:4.3f})'\n",
    "                \n",
    "                # （valid_loader が吐き出す最初のバッチのみ）val の実際の出力データを SummaryWriter に流す\n",
    "                if j == 0:\n",
    "                    for k in range(min(10, tr_cfg.val_batch_size)):\n",
    "                        spkr = speakers[k]\n",
    "                        fpath = filenames[k]\n",
    "                        fname = Path(fpath).name\n",
    "                        if first_step == True:\n",
    "                            writer.add_audio(\n",
    "                                f'spkr-{spkr}_{fname}/x_(ground_truth)', \n",
    "                                wav_orig[k, :].detach().cpu(), \n",
    "                                epoch, \n",
    "                                tr_cfg.sr,\n",
    "                            )\n",
    "                            writer.add_figure(\n",
    "                                f'spkr-{spkr}_{fname}/y_(ground_truth)', \n",
    "                                plot_spectrogram_harmof0(\n",
    "                                    spec[k, 48:, :].detach().cpu().numpy(),\n",
    "                                    f0 = F0_real[k, :].detach().cpu(), \n",
    "                                    act = act_t[k, :].detach().cpu(), \n",
    "                                ), \n",
    "                                epoch,\n",
    "                            )\n",
    "                            \n",
    "                        writer.add_audio(\n",
    "                            f'spkr-{spkr}_{fname}/xhat_(VC)', \n",
    "                            reconst[k, 0, :].detach(), \n",
    "                            epoch, \n",
    "                            tr_cfg.sr,\n",
    "                        )\n",
    "                        # 復元した wav (24k) から再度スペクトログラムを計算\n",
    "                        w16_recon = resample_orig_to16(reconst.to(device))\n",
    "                        F0_recon, _, _, spec_recon = harmof0_tracker(w16_recon) \n",
    "                        writer.add_figure(\n",
    "                            f'spkr-{spkr}_{fname}/yhat_(VC)',\n",
    "                            plot_spectrogram_harmof0(\n",
    "                                spec_recon[k, 0, 48:, :].detach().cpu().numpy(),\n",
    "                                f0 = F0_fake[k, :].detach().cpu(), \n",
    "                            ), \n",
    "                            epoch,\n",
    "                        )\n",
    "                first_step = False # 初回 validation step の処理が終わった\n",
    "        style_encoder.train() # 保存後、訓練モードに戻す\n",
    "        f0n_predictor.train()\n",
    "        decoder.train()\n",
    "\n",
    "        with open(path_remote_log, mode = 'a') as f:\n",
    "            f.write(\n",
    "                \"[{}] Val spectrogram loss: {:5.4f} (epoch {:4d}, step {:7d})\\n\".format(\n",
    "                    str(datetime.now()), avg_val_loss, epoch, step\n",
    "                ),\n",
    "            )\n",
    "        # TensorBoard への valid 結果の書き込み\n",
    "        if avg_val_loss > 0:\n",
    "            writer.add_scalars(\n",
    "                'Loss/00_spec_loss', {\n",
    "                    \"val\": avg_val_loss.item()\n",
    "                }, step\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                'Loss/10_F0', {\n",
    "                    \"val\": avg_val_F0.item()\n",
    "                }, step\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                'Loss/20_Energy', {\n",
    "                    \"val\": avg_val_norm.item()\n",
    "                }, step\n",
    "            )\n",
    "        writer.add_scalar(\"Monitor/max_memory_allocated\", torch.cuda.max_memory_allocated() / 1e9, step)\n",
    "        logging.info(f\"Validation spec loss at epoch {epoch:10d}: {avg_val_loss:5.4f}\")\n",
    "\n",
    "    # tr_cfg.save_every ごとに、再学習が可能なフルセットの state_dict を保存する\n",
    "    \n",
    "    if epoch % tr_cfg.save_every == 0 and epoch != last_epoch:\n",
    "        save_path = ckpt_path / f\"vc_1st_{epoch:08d}.pt\"\n",
    "        style_encoder.cpu()\n",
    "        f0n_predictor.cpu()\n",
    "        decoder.cpu()\n",
    "        torch.save(\n",
    "            {\n",
    "                'style_encoder': style_encoder.state_dict(),\n",
    "                'f0n_predictor': f0n_predictor.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'optim_g': optimizer_g.state_dict(),\n",
    "                'optim_do': optimizer_do.state_dict(),\n",
    "                'mpd': mpd.state_dict(),\n",
    "                'msd': msd.state_dict(),\n",
    "                'wd': wd.state_dict(),\n",
    "                'schedule_g': scheduler_g.state_dict(),\n",
    "                'schedule_do': scheduler_do.state_dict(),\n",
    "                'step': step,\n",
    "                'epoch': epoch, \n",
    "            }, \n",
    "            save_path,\n",
    "        )\n",
    "        style_encoder.to(device)\n",
    "        f0n_predictor.to(device)\n",
    "        decoder.to(device)\n",
    "\n",
    "    #### ここから各ステップの順伝播\n",
    "    \n",
    "    # DataLoader をラップしたプログレスバーで、訓練データセットから batch_size 話者、n_utterances 個の spec データを振り出す\n",
    "    for i, batch in pb:\n",
    "        start_b = time.time()\n",
    "        wav_orig = batch[0]\n",
    "        filenames = batch[1] # [\"path\", \"path\", \"path\", \"path\"]\n",
    "        speakers = batch[2] # [\"name\", \"name\", \"name\", \"name\"]\n",
    "        # 参照用に、同じ話者の別の発話も取り出す。追加のスタイル損失の計算に使う\n",
    "        ref_wav_orig = batch[3]\n",
    "        ref_filenames = batch[4] # [\"path\", \"path\", \"path\", \"path\"]\n",
    "\n",
    "        #### 順伝播と損失の計算\n",
    "        \n",
    "        w16 = resample_orig_to16(wav_orig.to(device))\n",
    "        with torch.no_grad():\n",
    "            F0_real, act_t, N_real, spec = harmof0_tracker(w16)\n",
    "        s_a = style_encoder(spec[:, 48:, :].unsqueeze(1))\n",
    "        with torch.no_grad():\n",
    "            content = CE(w16)[\"last_hidden_state\"].transpose(2, 1)\n",
    "                \n",
    "        # スタイル損失の計算用に、別の発話の s_a だけ計算しておく。\n",
    "        with torch.no_grad():\n",
    "            ref_w16 = resample_orig_to16(ref_wav_orig.to(device))\n",
    "            _, _, _, ref_spec = harmof0_tracker(ref_w16)\n",
    "            ref_s_a = style_encoder(ref_spec[:, 48:, :].unsqueeze(1)) \n",
    "\n",
    "        F0_fake, N_fake = f0n_predictor(\n",
    "            content, \n",
    "            s_a,\n",
    "        )\n",
    "\n",
    "        # decoder に長い音声を突っ込むと VRAM が足りないので、末尾からのランダムな開始場所で切り出した区間を使う\n",
    "        decode_len = content.size(2) // 2 # 切り出す長さは元クリップの半分\n",
    "        sample_from = random.randint(1, decode_len) # もし 0 が入ると decoder のスライスが狂うので 1 以上\n",
    "        # 正しい F0 ベースの復元音声もしくは予想した F0 ベースの復元音声を作る。いずれも s_a をスタイルに使う\n",
    "        if epoch >= tr_cfg.TMA_epoch and random.randint(0, 1) >= 1:\n",
    "            reconst = decoder(\n",
    "                content[:, :, -(sample_from+decode_len):-sample_from], \n",
    "                F0_fake[:, -2*(sample_from+decode_len):-2*sample_from], \n",
    "                N_fake[:, -2*(sample_from+decode_len):-2*sample_from], \n",
    "                s_a,\n",
    "            ) # 再構成した音声は singleton dimension が入った (batch, 1, time*) \n",
    "        else:\n",
    "            reconst = decoder(\n",
    "                content[:, :, -(sample_from+decode_len):-sample_from], \n",
    "                F0_real[:, -2*(sample_from+decode_len):-2*sample_from], \n",
    "                N_real[:, -2*(sample_from+decode_len):-2*sample_from], \n",
    "                s_a,\n",
    "            )\n",
    "\n",
    "        # wav_orig は損失計算までに device に送っておく\n",
    "        wav_orig = wav_orig[:, -decoder_cfg.upsample_total*(sample_from+decode_len):-decoder_cfg.upsample_total*sample_from].to(device) \n",
    "        \n",
    "        ####\n",
    "\n",
    "        # discriminator loss\n",
    "        if epoch >= tr_cfg.TMA_epoch:\n",
    "            optimizer_do.zero_grad()\n",
    "            d_loss = dl(wav_orig.detach().unsqueeze(1).float(), reconst.detach()).mean() # こちらは fake ベース\n",
    "            # 逆伝播→ unscale_() → clip_grad_norm_() → step() → update() の順\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.unscale_(optimizer_do)\n",
    "            _ = clip_grad_norm_( mpd.parameters(), max_norm = tr_cfg.grad_clip, norm_type = 2.0, )\n",
    "            _ = clip_grad_norm_( msd.parameters(), max_norm = tr_cfg.grad_clip, norm_type = 2.0, )\n",
    "            _ = clip_grad_norm_( wd.parameters(), max_norm = tr_cfg.grad_clip, norm_type = 2.0, )\n",
    "            scaler.step(optimizer_do) # scaler.step() は勾配の Inf や NaN をチェックし、問題なければ optimizer.step() を呼ぶ\n",
    "            scaler.update() # 次のイテレーションに入るまでに、scaler をアップデート\n",
    "        else:\n",
    "            d_loss = 0\n",
    "\n",
    "        # generator loss\n",
    "        optimizer_g.zero_grad()\n",
    "        loss_spec = stft_loss(reconst.squeeze(1), wav_orig[:, -reconst.shape[-1]:].detach())\n",
    "        # F0 loss ここを、「activation map の値が大きいときだけ損失定義」として改造した\n",
    "        act_binary = (act_t[:, -content.size(2)*2:] >= tr_cfg.f0_act_threshold).float()\n",
    "        loss_F0_rec =  F.smooth_l1_loss(\n",
    "            act_binary * F0_real.detach()[:, -content.size(2)*2:], \n",
    "            act_binary * F0_fake[:, -content.size(2)*2:]\n",
    "        )\n",
    "        loss_norm_rec = F.smooth_l1_loss(N_real[:, -content.size(2)*2:].detach(), N_fake[:, -content.size(2)*2:]) # Energy 予測値の差        \n",
    "\n",
    "        loss_sty = F.l1_loss(s_a.detach(), ref_s_a)\n",
    "        \n",
    "        if epoch >= tr_cfg.TMA_epoch: # エポックが TMA_epoch 以上にならないと TMA が開始されない\n",
    "            loss_gen_all = gl(wav_orig.detach().unsqueeze(1).float(), reconst).mean() # stft loss は第 1 引数が reconst で gl, wl は逆\n",
    "            loss_slm = wl(wav_orig.detach(), reconst.squeeze(1)).mean()\n",
    "            g_loss = tr_cfg.lambda_spec * loss_spec + \\\n",
    "                tr_cfg.lambda_F0 * loss_F0_rec + \\\n",
    "                tr_cfg.lambda_norm * loss_norm_rec + \\\n",
    "                tr_cfg.lambda_sty * loss_sty + \\\n",
    "                tr_cfg.lambda_gen * loss_gen_all + \\\n",
    "                tr_cfg.lambda_slm * loss_slm\n",
    "        else:\n",
    "            loss_gen_all = 0\n",
    "            loss_slm = 0\n",
    "            g_loss = tr_cfg.lambda_spec * loss_spec + \\\n",
    "                tr_cfg.lambda_F0 * loss_F0_rec + \\\n",
    "                tr_cfg.lambda_norm * loss_norm_rec + \\\n",
    "                tr_cfg.lambda_sty * loss_sty\n",
    "        \n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.unscale_(optimizer_g)\n",
    "        _ = clip_grad_norm_( style_encoder.parameters(), max_norm = tr_cfg.grad_clip, norm_type = 2.0, )\n",
    "        _ = clip_grad_norm_( decoder.parameters(), max_norm = tr_cfg.grad_clip, norm_type = 2.0, )\n",
    "        scaler.step(optimizer_g)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 子プログレスバーのコメントを更新\n",
    "        mb.child.comment = '(step {:09d}, spectrogram loss: {:03.3f}, {:03.3f} steps/s)'. \\\n",
    "                format(step, loss_spec.item(), 1 / (time.time() - start_b) )\n",
    "\n",
    "        if step % tr_cfg.board_every == 0:\n",
    "            with torch.no_grad():\n",
    "                writer.add_scalars(\n",
    "                    'Loss/00_spec_loss', {\n",
    "                        \"train\": loss_spec.item()\n",
    "                    }, step\n",
    "                )\n",
    "                writer.add_scalars(\n",
    "                    'Loss/10_F0', {\n",
    "                        \"train\": loss_F0_rec.item()\n",
    "                    }, step\n",
    "                )\n",
    "                writer.add_scalars(\n",
    "                    'Loss/20_Energy', {\n",
    "                        \"train\": loss_norm_rec.item()\n",
    "                    }, step\n",
    "                )\n",
    "                if epoch >= tr_cfg.TMA_epoch: \n",
    "                    writer.add_scalar(\"Loss/30_train_gen_all\", loss_gen_all.item(), step)\n",
    "                    writer.add_scalar(\"Loss/40_train_slm\", loss_slm.item(), step)\n",
    "                writer.add_scalar(\"Loss/50_train_sty\", loss_sty.item(), step)\n",
    "                writer.add_scalar(\"Monitor/lr\", optimizer_g.param_groups[0][\"lr\"], step)\n",
    "                writer.add_scalar(\"Monitor/max_memory_allocated\", torch.cuda.max_memory_allocated() / 1e9, step)\n",
    "        torch.cuda.empty_cache()\n",
    "        step += 1 # オプティマイザとは関係なくユーザーが作った、現在のステップを保持する変数をインクリメント\n",
    "\n",
    "    # 以下は各エポックの全 step 投入後に行う処理\n",
    "    scheduler_g.step(epoch + 1) # 学習率の更新。更新単位は通常は epoch\n",
    "    scheduler_do.step(epoch + 1)\n",
    "\n",
    "# 当初予定のエポックが全て終了したら値を保存する\n",
    "save_path = ckpt_path / f\"vc_1st_{epoch:08d}.pt\"\n",
    "style_encoder.eval().cpu()\n",
    "f0n_predictor.eval().cpu()\n",
    "decoder.eval().cpu()\n",
    "msd.eval()\n",
    "mpd.eval()\n",
    "torch.save(\n",
    "    {\n",
    "        'style_encoder': style_encoder.state_dict(),\n",
    "        'f0n_predictor': f0n_predictor.state_dict(),\n",
    "        'decoder': decoder.state_dict(),\n",
    "        'optim_g': optimizer_g.state_dict(),\n",
    "        'optim_do': optimizer_do.state_dict(),\n",
    "        'mpd': mpd.state_dict(),\n",
    "        'msd': msd.state_dict(),\n",
    "        'wd': wd.state_dict(),\n",
    "        'schedule_g': scheduler_g.state_dict(),\n",
    "        'schedule_do': scheduler_do.state_dict(),\n",
    "        'step': step,\n",
    "        'epoch': epoch, \n",
    "    }, \n",
    "    save_path,\n",
    ")\n",
    "\n",
    "logging.info(f\"Epoch {epoch:10d} ended.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
