{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087d9136",
   "metadata": {},
   "source": [
    "# ContentVec を transformers ベースで使えるようにする\n",
    "\n",
    "Lyodos 著\n",
    "\n",
    "Version 1.0.0 (2024-07-14)\n",
    "\n",
    "> このノートブックにある手順は、 https://huggingface.co/lengyue233/content-vec-best/blob/main/convert.py による。\n",
    "\n",
    "ContentVec の重みは、公式で配布されているもの（ContentVec_legacy 500 と書いてあるリンク）を落とす。\n",
    "\n",
    "https://github.com/auspicious3000/contentvec\n",
    "\n",
    "https://ibm.ent.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr\n",
    "\n",
    "ただし公式のネットワーク定義だと ONNX 化した際に不具合が出るので、transformers パッケージに準拠して HuBERT を作り、\n",
    "そちらに重みを移植する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326f5a4",
   "metadata": {},
   "source": [
    "\n",
    "> なお、ここからの作業中に以下のエラーが出る場合は protobuf のバージョンが tensorflow に対して新しすぎるので、TF 自体を上げるか、protobuf をダウングレードするか、あるいは ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python' を設定する。\n",
    "\n",
    "```\n",
    "TypeError: Descriptors cannot be created directly.\n",
    "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
    "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
    " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
    " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
    "\n",
    "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215ee2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:03:41.311594Z",
     "start_time": "2024-07-14T14:03:41.306273Z"
    }
   },
   "outputs": [],
   "source": [
    "# ただしこの環境変数を入れると重くなる\n",
    "import os\n",
    "\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576245c2",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### まず ContentVec のモデルの重みをロードしてみる\n",
    "\n",
    "落としたファイルを \"checkpoint_best_legacy_500.pt\" として以下のフォルダに配置したものとする。\n",
    "ここから、まず `fairseq` の方式でモデルをタスクとしてロードする。\n",
    "\n",
    "> 蛇足：最初は混乱するのだが、`ContentVec` という名前のゼロから開発された新規構造のモデルがあるわけではない。\n",
    "PyTorch ベースで系列モデルを扱うための `fairseq` というツールキットがあり、`ContentVec` のなかでも \"legacy\" と書いてある重みについては、\n",
    "このツールキットに従う形でモデルを初期化して利用できる。\n",
    "> また `ContentVec` のモデル構造としては、`fairseq` を通じて提供されている様々なモデル構造の中でも `HuBERT` を採用している。\n",
    "> ただし `fairseq` で提供される `HuBERT` は、ネットワークの定義方法の都合で ONNX に書き出すと不具合が生じる。\n",
    "> なので、同様に `HuBERT` 構造を提供しているツールキットであり、かつ ONNX に書き出せる形で定義コードが書かれている `transformers` を使う。\n",
    ">  `fairseq` 用に作られた重み（チェックポイント）の辞書をそのまま `transformers` で読むことは困難なので、辞書の項目名を書き換えてやるわけだ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b74c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:05:09.933523Z",
     "start_time": "2024-07-14T14:05:08.555141Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fairseq import checkpoint_utils # なければ https://pypi.org/project/fairseq/\n",
    "\n",
    "\n",
    "DATASET_ROOT_PATH = Path(\"/home/lyodos/study/dataset\") # このフォルダ名はユーザーの実情に合わせて書き変えること\n",
    "\n",
    "best_legacy_500_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"checkpoint_best_legacy_500.pt\"\n",
    "\n",
    "models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n",
    "    [str(best_legacy_500_path)], \n",
    "    suffix = \"\",\n",
    ")\n",
    "\n",
    "model = models[0]\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252daa8",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 辞書の移植用に一時的に使用する HubertModelWithFinalProj モデルクラスの定義\n",
    "\n",
    "次に、transformers パッケージが提供する HubertModel に基づいて、新規のモデルクラス HubertModelWithFinalProj を定義する。\n",
    "\n",
    "HubertModel の元定義は以下にある。\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/hubert/modeling_hubert.py#L1330\n",
    "\n",
    "HubertModelWithFinalProj は、HubertModel に self.final_proj という層を足したものである。\n",
    "ContentVec 公式で配布されているモデルは、この final_proj という線形層を持つが、`transformers` パッケージで定義されている構造には存在しない。\n",
    "\n",
    "* ちなみに `__init__` で追加しているだけであって `forward` を書き換えていないので、いわゆる orphan であり推論結果には影響しない。\n",
    "\n",
    "* 重みを移植するとき層の構造が違うぞと叱られないためだけに、つじつま合わせに付けている。\n",
    "\n",
    "* 最終的には ClassicVC ではこのネットワーク定義は使用しない。\n",
    "`final_proj` は不要なので、そのまま `transformers` の `HubertModel` を使うだけ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a2444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:05:22.520187Z",
     "start_time": "2024-07-14T14:05:21.528548Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import HubertConfig, HubertModel\n",
    "from torch import nn\n",
    "\n",
    "class HubertModelWithFinalProj(HubertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)\n",
    "\n",
    "# デフォルトの config でインスタンス化\n",
    "hubert = HubertModelWithFinalProj(HubertConfig())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e8e0e",
   "metadata": {},
   "source": [
    "\n",
    "すでに作ってある model から、新しく作った hubert に重みを移植する。\n",
    "まず mapping として、層名を読み替える辞書（新：旧）を作る。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906cd98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:05:24.392567Z",
     "start_time": "2024-07-14T14:05:24.387003Z"
    }
   },
   "outputs": [],
   "source": [
    "# huggingface <- fairseq\n",
    "mapping = {\n",
    "    \"masked_spec_embed\": \"mask_emb\",\n",
    "    \"encoder.layer_norm.bias\": \"encoder.layer_norm.bias\",\n",
    "    \"encoder.layer_norm.weight\": \"encoder.layer_norm.weight\",\n",
    "    \"encoder.pos_conv_embed.conv.bias\": \"encoder.pos_conv.0.bias\",\n",
    "    \"encoder.pos_conv_embed.conv.weight_g\": \"encoder.pos_conv.0.weight_g\",\n",
    "    \"encoder.pos_conv_embed.conv.weight_v\": \"encoder.pos_conv.0.weight_v\",\n",
    "    \"feature_projection.layer_norm.bias\": \"layer_norm.bias\",\n",
    "    \"feature_projection.layer_norm.weight\": \"layer_norm.weight\",\n",
    "    \"feature_projection.projection.bias\": \"post_extract_proj.bias\",\n",
    "    \"feature_projection.projection.weight\": \"post_extract_proj.weight\",\n",
    "    \"final_proj.bias\": \"final_proj.bias\",\n",
    "    \"final_proj.weight\": \"final_proj.weight\",\n",
    "}\n",
    "\n",
    "# Convert encoder\n",
    "for layer in range(12):\n",
    "    for j in [\"q\", \"k\", \"v\"]:\n",
    "        mapping[\n",
    "            f\"encoder.layers.{layer}.attention.{j}_proj.weight\"\n",
    "        ] = f\"encoder.layers.{layer}.self_attn.{j}_proj.weight\"\n",
    "        mapping[\n",
    "            f\"encoder.layers.{layer}.attention.{j}_proj.bias\"\n",
    "        ] = f\"encoder.layers.{layer}.self_attn.{j}_proj.bias\"\n",
    "\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.final_layer_norm.bias\"\n",
    "    ] = f\"encoder.layers.{layer}.final_layer_norm.bias\"\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.final_layer_norm.weight\"\n",
    "    ] = f\"encoder.layers.{layer}.final_layer_norm.weight\"\n",
    "\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.layer_norm.bias\"\n",
    "    ] = f\"encoder.layers.{layer}.self_attn_layer_norm.bias\"\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.layer_norm.weight\"\n",
    "    ] = f\"encoder.layers.{layer}.self_attn_layer_norm.weight\"\n",
    "\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.attention.out_proj.bias\"\n",
    "    ] = f\"encoder.layers.{layer}.self_attn.out_proj.bias\"\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.attention.out_proj.weight\"\n",
    "    ] = f\"encoder.layers.{layer}.self_attn.out_proj.weight\"\n",
    "\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.feed_forward.intermediate_dense.bias\"\n",
    "    ] = f\"encoder.layers.{layer}.fc1.bias\"\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.feed_forward.intermediate_dense.weight\"\n",
    "    ] = f\"encoder.layers.{layer}.fc1.weight\"\n",
    "\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.feed_forward.output_dense.bias\"\n",
    "    ] = f\"encoder.layers.{layer}.fc2.bias\"\n",
    "    mapping[\n",
    "        f\"encoder.layers.{layer}.feed_forward.output_dense.weight\"\n",
    "    ] = f\"encoder.layers.{layer}.fc2.weight\"\n",
    "\n",
    "# Convert Conv Layers\n",
    "for layer in range(7):\n",
    "    mapping[\n",
    "        f\"feature_extractor.conv_layers.{layer}.conv.weight\"\n",
    "    ] = f\"feature_extractor.conv_layers.{layer}.0.weight\"\n",
    "\n",
    "    if layer != 0:\n",
    "        continue\n",
    "\n",
    "    mapping[\n",
    "        f\"feature_extractor.conv_layers.{layer}.layer_norm.weight\"\n",
    "    ] = f\"feature_extractor.conv_layers.{layer}.2.weight\"\n",
    "    mapping[\n",
    "        f\"feature_extractor.conv_layers.{layer}.layer_norm.bias\"\n",
    "    ] = f\"feature_extractor.conv_layers.{layer}.2.bias\"\n",
    "\n",
    "hf_keys = set(hubert.state_dict().keys())\n",
    "fair_keys = set(model.state_dict().keys())\n",
    "\n",
    "hf_keys -= set(mapping.keys())\n",
    "fair_keys -= set(mapping.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49adfb4",
   "metadata": {},
   "source": [
    "\n",
    "上で作った対照表に基づき、HubertModelWithFinalProj インスタンスに重みを移植していく。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a341b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:05:26.334973Z",
     "start_time": "2024-07-14T14:05:26.209324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try loading the weights\n",
    "new_state_dict = {}\n",
    "\n",
    "for k, v in mapping.items():\n",
    "    new_state_dict[k] = model.state_dict()[v]\n",
    "\n",
    "x = hubert.load_state_dict(new_state_dict, strict = False)\n",
    "print(x)\n",
    "hubert.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1d6a3",
   "metadata": {},
   "source": [
    "\n",
    "この方法で作ったモデルが、通常通り順伝播できて、しかも ONNX 化の可能な ContentVec モデルになる。\n",
    "ただし最終隠れ層の状態を取り出して VC に使おうとすると、\n",
    "単なる順伝播の返り値でなく `[\"last_hidden_state\"]` という辞書にアクセスする必要がある。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fed5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:05:40.199561Z",
     "start_time": "2024-07-14T14:05:39.615318Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchinfo\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# VCTK コーパスからのボイスサンプルを読み込んでみる\n",
    "with torch.no_grad():\n",
    "    waveform, orig_sr = torchaudio.load('../wavs/p225_003.wav') # まずオリジナル周波数でロード\n",
    "    wav16 = torchaudio.transforms.Resample(orig_freq = orig_sr, new_freq = 16000)(waveform).to(device)\n",
    "    %time content = hubert.to(device)(wav16, output_hidden_states = True)\n",
    "\n",
    "print(type(content))\n",
    "print(len(content))\n",
    "print(content.keys())\n",
    "print(content[\"last_hidden_state\"].shape)\n",
    "\n",
    "torchinfo.summary(model = hubert, input_size = wav16.shape, depth = 4, \n",
    "                  col_names=[\"input_size\", \"output_size\", \"num_params\"], device = \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e61fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:05:46.148700Z",
     "start_time": "2024-07-14T14:05:46.047496Z"
    }
   },
   "outputs": [],
   "source": [
    "# 変換されたモデルと元のモデルの出力の比較\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_input = torch.randn(1, 16384)\n",
    "\n",
    "    # こちらは hubert の順伝播。返り値のうち、隠れ層の 9 番目が必要。\n",
    "    result1 = hubert(new_input, output_hidden_states = True)[\"hidden_states\"][9]\n",
    "    # 隠れ層の 9 番目で\n",
    "    result1 = hubert.final_proj(result1)\n",
    "\n",
    "    # こちらが古いモデル。\n",
    "    result2 = model.extract_features(\n",
    "        **{\n",
    "            \"source\": new_input,\n",
    "            \"padding_mask\": torch.zeros(1, 16384, dtype=torch.bool),\n",
    "            # \"features_only\": True,\n",
    "            \"output_layer\": 9,\n",
    "        }\n",
    "    )[0]\n",
    "    result2 = model.final_proj(result2)\n",
    "\n",
    "    assert torch.allclose(result1, result2, atol=1e-3)\n",
    "\n",
    "print(\"Sanity check passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fcf014",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 重みの保存\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118a80f",
   "metadata": {},
   "source": [
    "\n",
    "では、この定義部分だけをどうやって取り出すか。\n",
    "\n",
    "実は Hubert クラス（HubertModelWithFinalProj ではなく）をそのままインスタンス化するだけでいい。\n",
    "\n",
    "ここに上の hubert インスタンスの state dict をロードして、それからローカルに保存する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c751f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:06:31.200522Z",
     "start_time": "2024-07-14T14:06:30.517995Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import HubertConfig, HubertModel\n",
    "\n",
    "CE = HubertModel(HubertConfig())\n",
    "\n",
    "CE_dict = hubert.cpu().state_dict() # 重みだけ取り出す\n",
    "CE.load_state_dict(CE_dict, strict = False) # 新しいモデルインスタンスにロード\n",
    "CE.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e4cd3",
   "metadata": {},
   "source": [
    "\n",
    "テストしてみる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1244492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:06:51.409514Z",
     "start_time": "2024-07-14T14:06:50.488905Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    waveform, orig_sr = torchaudio.load('../wavs/p225_003.wav') # まずオリジナル周波数でロード\n",
    "    wav16 = torchaudio.transforms.Resample(orig_freq = orig_sr, new_freq = 16000)(waveform).to(device)\n",
    "    %time content = CE.to(device)(wav16, output_hidden_states = True)\n",
    "\n",
    "print(type(content))\n",
    "print(len(content))\n",
    "print(content.keys())\n",
    "print(content[\"last_hidden_state\"].shape)\n",
    "\n",
    "torchinfo.summary(model = CE, input_size = wav16.shape, depth = 4, \n",
    "                  col_names=[\"input_size\", \"output_size\", \"num_params\"], device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bcce9",
   "metadata": {},
   "source": [
    "\n",
    "重みをファイルに保存する。\n",
    "訓練時はこの PyTorch 用の重みをロードして使う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b62ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:07:15.726714Z",
     "start_time": "2024-07-14T14:07:15.724918Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"contentvec_500_hubert.pth\"\n",
    "\n",
    "torch.save(\n",
    "    CE.cpu().state_dict(),\n",
    "    save_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef4f91",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "### ONNX 化\n",
    "\n",
    "この CE は通常の方法で ONNX に export できる。なお `dynamic_axes` で可変長時系列に対応させる必要がある。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensor_x = torch.rand((1, 16000*4), dtype = torch.float32) * 0.2\n",
    "\n",
    "onnx_path = DATASET_ROOT_PATH / \"checkpoints\" / \"classic-vc\" / \"hubert500.onnx\"\n",
    "\n",
    "# ONNX形式にエクスポート\n",
    "torch.onnx.export(\n",
    "    CE, \n",
    "    tensor_x, \n",
    "    onnx_path, \n",
    "    opset_version = 17,\n",
    "    input_names=['input'],\n",
    "    output_names=['last_hidden_state'],\n",
    "    dynamic_axes = {\n",
    "        'input': {0: 'batch', 1: 'samples'},\n",
    "        'last_hidden_state': {0: 'batch', 1: 'frames'}\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "audio_file = '../wavs/p225_003.wav'\n",
    "y, sr = librosa.load(audio_file, sr = None, mono = False)\n",
    "y16 = librosa.resample(y, orig_sr = sr, target_sr = 16000)\n",
    "\n",
    "if y.ndim == 1:  # モノラル\n",
    "    y16 = y16.reshape(1, -1)\n",
    "elif y.ndim == 2:  # ステレオ\n",
    "    y16 = y16.T\n",
    "\n",
    "print(y16.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f08260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "sess = onnxruntime.InferenceSession(\n",
    "    onnx_path, \n",
    "    providers  =[\n",
    "        'CUDAExecutionProvider', \n",
    "        'CPUExecutionProvider',\n",
    "    ],\n",
    ")\n",
    "\n",
    "for i in sess.get_inputs():\n",
    "    print(\"Input name: '{}', type: {}, shape: {}\".format(i.name, i.shape, i.type))\n",
    "\n",
    "for o in sess.get_outputs():\n",
    "    print(\"Output name: '{}', type: {}, shape: {}\".format(o.name, o.shape, o.type))\n",
    "\n",
    "%time content_onnx = sess.run(['last_hidden_state'], {\"input\": y16})[0]\n",
    "\n",
    "print(y16.shape, content_onnx.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(content_torch.squeeze(), vmin = -1, vmax = 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(content_onnx.squeeze(), vmin = -1, vmax = 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e284236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43756f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
